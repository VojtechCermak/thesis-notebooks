{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# file manipulation\n",
    "import os\n",
    "\n",
    "# word embedings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "#read lists\n",
    "from ast import literal_eval\n",
    "\n",
    "# scikit\n",
    "#load classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# laod vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# evaluation\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Functions\n",
    "# convert date, hours and minutes multiindex to single datetime index\n",
    "def index_to_datetime(inputDF, freq):\n",
    "    '''\n",
    "    Utility function converting \n",
    "\n",
    "    Creates datetime index. Prices are placed on date time grid with one minute frequency\n",
    "    in order to cover After-Hours price changes. \n",
    "\n",
    "    '''\n",
    "    dataFrame = inputDF.copy()\n",
    "    dataFrame = dataFrame.reset_index()\n",
    "    \n",
    "    if freq == 'min':\n",
    "        dataFrame['DateTime'] = dataFrame['date'] + ' ' + dataFrame['hour'].astype(str) + ':' + dataFrame['minute'].astype(str)\n",
    "        dataFrame = dataFrame.drop(['date', 'hour', '5min', 'minute'], axis=1)\n",
    "    elif freq == '5min':\n",
    "        dataFrame['DateTime'] = dataFrame['date'] + ' ' + dataFrame['hour'].astype(str) + ':' + dataFrame['5min'].astype(str)\n",
    "        dataFrame = dataFrame.drop(['date', 'hour', '5min'], axis=1)\n",
    "    elif freq == 'hour':\n",
    "        dataFrame['DateTime'] = dataFrame['date'] + ' ' + dataFrame['hour'].astype(str)\n",
    "        dataFrame = dataFrame.drop(['date', 'hour'], axis=1)        \n",
    "    else:\n",
    "        print('Unsupported frequency')\n",
    "        return\n",
    "    \n",
    "    dataFrame['DateTime'] = pd.to_datetime(dataFrame['DateTime'])    \n",
    "    dataFrame = dataFrame.set_index('DateTime')\n",
    "    return dataFrame\n",
    "\n",
    "# loads market data and converts to suitable format\n",
    "def load_marketdata(path):\n",
    "    stockDF = pd.read_csv(path)\n",
    "    stockDF['DateTime'] = stockDF['Date'] + ' ' + stockDF['Time']\n",
    "    stockDF['DateTime'] = pd.to_datetime(stockDF['DateTime'])\n",
    "    stockDF = stockDF.set_index('DateTime')\n",
    "    return stockDF\n",
    "\n",
    "def load_marketdata(path, add_grid = True):\n",
    "\n",
    "    # Load data\n",
    "    stockDF = pd.read_csv(path)\n",
    "    stockDF['DateTime'] = stockDF['Date'] + ' ' + stockDF['Time']\n",
    "    stockDF['DateTime'] = pd.to_datetime(stockDF['DateTime'])\n",
    "    stockDF = stockDF.set_index('DateTime')\n",
    "    \n",
    "    if add_grid:\n",
    "        # Create grid\n",
    "        grid_start = min(stockDF.index) - pd.DateOffset(days=5)\n",
    "        grid_end = max(stockDF.index) + pd.DateOffset(days=5)\n",
    "        grid   = load_grid(start = grid_start, end = grid_end)\n",
    "        \n",
    "        # Join grid with data\n",
    "        stockDF = grid.join(stockDF)\n",
    "        was_NaN = stockDF['Close'].isnull()\n",
    "        stockDF = stockDF.fillna(method = 'ffill')\n",
    "        stockDF['was_NaN'] = was_NaN\n",
    "    \n",
    "    return stockDF\n",
    "\n",
    "# creates time grid with suitable format\n",
    "def load_grid(start, end, freq='min'):\n",
    "    grid = pd.date_range(start=start, end=end, freq=freq)\n",
    "    grid = pd.Series(grid).rename('DateTime')\n",
    "    grid = pd.DataFrame(grid).set_index('DateTime')\n",
    "    return grid\n",
    "\n",
    "def load_tweets(path):\n",
    "    tweets = pd.read_csv(path)\n",
    "    # convert column values to lists of words\n",
    "    tweets['lemmas'] = tweets['lemmas'].apply(literal_eval)\n",
    "    tweets['tokens'] = tweets['tokens'].apply(literal_eval)\n",
    "    \n",
    "    # create time variables\n",
    "    tweets['created_at'] = pd.to_datetime(tweets['created_at'], format='%Y-%m-%d %H:%M:%S')\n",
    "    tweets['date'] = tweets['created_at'].astype(str).str[:10]\n",
    "    tweets['hour'] = tweets['created_at'].astype(str).str[11:13]\n",
    "    tweets['minute'] = tweets['created_at'].astype(str).str[14:16]\n",
    "    tweets['5min'] = (tweets['minute'].astype(int)//5)*5\n",
    "\n",
    "    #Spam filtering - Remove duplicate tweets in date\n",
    "    tweets = tweets.drop_duplicates(['date', 'text'])\n",
    "\n",
    "    # Indexing\n",
    "    tweets.set_index(['date', 'hour', '5min' ,'minute', 'id'], inplace = True)\n",
    "    return tweets\n",
    "\n",
    "def aggregate_tweets(inputDF, freq, forms):\n",
    "    tweets = inputDF.copy()\n",
    "    special = ['F_exclamation', 'F_question', 'F_ellipsis', 'F_hashtags', 'F_cashtags', 'F_usermention', 'F_urls']\n",
    "    \n",
    "    if freq == 'min':\n",
    "        level = ['date', 'hour', '5min', 'minute']\n",
    "    elif freq == '5min':\n",
    "        level = ['date', 'hour', '5min']\n",
    "    elif freq == 'hour':\n",
    "        level = ['date', 'hour']\n",
    "    elif freq == 'none':\n",
    "        level = ['date', 'hour', '5min', 'minute', 'id']\n",
    "        freq = 'min'\n",
    "    else:\n",
    "        print('Unsupported frequency') \n",
    "        return\n",
    "    \n",
    "    sum_text = tweets[forms].groupby(level=level).apply(sum)\n",
    "    sum_special = tweets[special].groupby(level=level).sum().add_prefix('sum')\n",
    "    avg_special = tweets[special].groupby(level=level).mean().add_prefix('avg')\n",
    "    count_tweets = tweets.groupby(level=level).size().rename('tweet_count')\n",
    "\n",
    "    finalDF = pd.concat([sum_special, avg_special, count_tweets, sum_text], axis = 1)\n",
    "    finalDF = finalDF.rename(columns={forms: \"text\"}) #rename lemmas/tokens to text\n",
    "    finalDF = index_to_datetime(finalDF, freq)\n",
    "    return finalDF\n",
    "\n",
    "def get_label(tweetDF, shift, biclass = True):\n",
    "    \"\"\"\n",
    "    shift = n  - label is n minutes lagged\n",
    "    shift = -n  - label is n minute in future\n",
    "    \"\"\"\n",
    "    df = grid.join(prices['Close'])\n",
    "    df = df.fillna(method = 'ffill')\n",
    "    \n",
    "    if shift > 0 :\n",
    "        df['minLag'] = df['Close'].shift(shift)\n",
    "        conditions = [df['minLag'] == df['Close'], df['minLag'] < df['Close'], df['minLag'] > df['Close']]\n",
    "        df['Label'] = np.select(conditions, ['NoChange', 'Growth', 'Decline'], default='Missing')\n",
    "    else:\n",
    "        df['minShift'] = df['Close'].shift(shift)\n",
    "        conditions = [df['minShift'] == df['Close'], df['minShift'] > df['Close'], df['minShift'] < df['Close']]\n",
    "        df['Label'] = np.select(conditions, ['NoChange', 'Growth', 'Decline'], default='Missing')\n",
    "        \n",
    "    finalDF = df.join(tweetDF)\n",
    "    finalDF = finalDF.dropna()\n",
    "    \n",
    "    # delete nochange labels if biclass TRUE\n",
    "    if biclass:\n",
    "        finalDF = finalDF[finalDF['Label'] != 'NoChange']\n",
    "    \n",
    "    return finalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load data\n",
    "tweets = load_tweets('C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataProcessed\\\\tweetsAAPL.csv')\n",
    "prices = load_marketdata('AAPL_1min.csv', add_grid = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature set 2: TF-IDF weighted Bag of Words features </h1>\n",
    "\n",
    "Steps:\n",
    "1. Lemmatize tokens\n",
    "2. Remove english stopwords\n",
    "3. take only 100 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(inputDF, labeling,  method, validations=5):\n",
    "    if method == 'logit':\n",
    "        model = LogisticRegression(C=1e30,penalty='l2')\n",
    "        pred = cross_val_predict(model, inputDF, labeling, cv=validations, n_jobs=1, verbose=0)\n",
    "        \n",
    "    elif method == 'L2_logit':\n",
    "        model = LogisticRegression(C=1, penalty='l2')\n",
    "        pred = cross_val_predict(model, inputDF, labeling, cv=validations, n_jobs=1, verbose=0)    \n",
    "        \n",
    "    elif method == 'L1_logit':\n",
    "        model = LogisticRegression(C=1, penalty='l1')\n",
    "        pred = cross_val_predict(model, inputDF, labeling, cv=validations, n_jobs=1, verbose=0)    \n",
    "        \n",
    "    elif method == 'nb':\n",
    "        model = MultinomialNB()\n",
    "        pred = cross_val_predict(model, inputDF, labeling, cv=validations, n_jobs=1, verbose=0)  \n",
    "    else:\n",
    "        raise ValueError('Method is not supported')\n",
    "        \n",
    "    return pred\n",
    "\n",
    "\n",
    "def get_metric(pred, label, method):\n",
    "    if method == 'kappa':\n",
    "        value = cohen_kappa_score(label, pred)\n",
    "    elif method == 'acc':\n",
    "        value = accuracy_score(label, pred)\n",
    "    else:\n",
    "        raise ValueError('Method is not supported')\n",
    "        \n",
    "    return value\n",
    "\n",
    "# List of vectorization methods\n",
    "def BOW_vectorize(inputText, method):\n",
    "    # COUNT VECTORIZER\n",
    "    # binary terms vectorizer\n",
    "    if method == 'binary':\n",
    "        vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, \n",
    "                              binary=True)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    # Simple count vectorizer\n",
    "    elif method == 'count':\n",
    "        vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, \n",
    "                              binary=False)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    # Simple count vectorizer with stopwords filter\n",
    "    elif method == 'count_sw':\n",
    "        vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, \n",
    "                              stop_words='english', binary=False)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    # TFIDF VECTORIZER\n",
    "    # Term frequencies vectorizer\n",
    "    elif method =='frequency':\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x,  \n",
    "                              sublinear_tf = False, use_idf=False)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    #simple TFIDF vectorizer\n",
    "    elif method =='tfidf':\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, \n",
    "                              sublinear_tf = False, use_idf=True)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    elif method =='tfidf_sw':\n",
    "        #simple TFIDF vectorizer with english stop words\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, \n",
    "                              stop_words='english',sublinear_tf = False, use_idf=True)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    elif method =='log_tfidf':\n",
    "        #LOG tf TFIDF vectorizer\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x,  \n",
    "                              sublinear_tf = True, use_idf=True)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    elif method =='log_tfidf_sw':\n",
    "        #LOG tf TFIDF vectorizer with english stop words\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, \n",
    "                              stop_words='english', sublinear_tf = True, use_idf=True)\n",
    "        train = vec.fit_transform(inputText)\n",
    "    else:\n",
    "        raise ValueError('Method is not supported')\n",
    "    return train\n",
    "\n",
    "def BOW_gridsearch(inputDict):\n",
    "    d = {}\n",
    "    # Create dataset\n",
    "    for form in inputDict['forms']:\n",
    "        d[form] = {}\n",
    "        for agg in inputDict['aggregates']:\n",
    "            d[form][agg] = {} \n",
    "\n",
    "            #create dataset based on values forms and aggregation methods\n",
    "            dataset = aggregate_tweets(tweets, agg, form)\n",
    "\n",
    "            # Add labels\n",
    "            for direction in inputDict['directions']:\n",
    "                d[form][agg][direction] = {}\n",
    "                for window in inputDict['windows']:\n",
    "                    d[form][agg][direction][window] = {}\n",
    "\n",
    "                    # get direction of window\n",
    "                    if direction == 'past':\n",
    "                        window_dir = window\n",
    "                    elif direction == 'future':\n",
    "                        window_dir = -1*window\n",
    "\n",
    "                    # Add label based on window to dataset\n",
    "                    labeled_dataset = get_label(dataset, window_dir)\n",
    "                    labeled_dataset = labeled_dataset.sample(frac=1) # shuffle\n",
    "\n",
    "                    text = labeled_dataset['text']\n",
    "                    label = labeled_dataset['Label']\n",
    "\n",
    "                    # create features using vectorizer\n",
    "                    for vec in inputDict['vectorizers']:\n",
    "                        d[form][agg][direction][window][vec] = {}\n",
    "                        features = BOW_vectorize(text, vec)\n",
    "                        print(form +' '+ agg +' '+ direction +' '+ str(window) +' '+ vec)\n",
    "\n",
    "                        # validate dataset using models and metrics\n",
    "                        for model in inputDict['models']:\n",
    "                            d[form][agg][direction][window][vec][model] = {}\n",
    "                            pred = get_model_prediction(features, label, model)\n",
    "                            for metric in inputDict['metrics']:\n",
    "                                value = get_metric(pred, label, metric)\n",
    "                                d[form][agg][direction][window][vec][model][metric] = value\n",
    "    return d\n",
    "\n",
    "def reform_BOW_gridsearch(inputDict):\n",
    "    reform = {(level1_key, level2_key, level3_key, level4_key, level5_key, level6_key, level7_key): values\n",
    "        for level1_key, level2_dict in inputDict.items()\n",
    "        for level2_key, level3_dict in level2_dict.items()\n",
    "        for level3_key, level4_dict in level3_dict.items()\n",
    "        for level4_key, level5_dict in level4_dict.items()\n",
    "        for level5_key, level6_dict in level5_dict.items()\n",
    "        for level6_key, level7_dict in level6_dict.items()\n",
    "        for level7_key, values      in level7_dict.items()}\n",
    "    dataFrame = pd.DataFrame(reform, index=[0]).T\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = ['lemmas', 'tokens']\n",
    "aggregates = ['hour', '5min', 'min', 'none']\n",
    "\n",
    "#labels\n",
    "directions = ['past', 'future']\n",
    "windows = [60, 1]\n",
    "\n",
    "vectorizers = ['binary', 'count', 'count_sw', 'frequency', 'tfidf', 'tfidf_sw', 'log_tfidf', 'log_tfidf_sw']\n",
    "\n",
    "# validation\n",
    "models = ['L2_logit', 'L1_logit', 'nb']\n",
    "metrics = ['kappa', 'acc']\n",
    "\n",
    "inputDict = {'forms':forms, 'aggregates':aggregates, 'directions':directions, \n",
    "             'windows':windows, 'vectorizers':vectorizers, 'models':models, 'metrics':metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = ['tokens']\n",
    "aggregates = ['min']\n",
    "\n",
    "#labels\n",
    "directions = ['future']\n",
    "windows = [1]\n",
    "\n",
    "vectorizers = ['log_tfidf_sw']\n",
    "\n",
    "# validation\n",
    "models = ['L2_logit', 'L1_logit', 'nb']\n",
    "metrics = ['kappa', 'acc']\n",
    "\n",
    "inputDict = {'forms':forms, 'aggregates':aggregates, 'directions':directions, \n",
    "             'windows':windows, 'vectorizers':vectorizers, 'models':models, 'metrics':metrics}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW_gridsearch(inputDict):\n",
    "    d = {}\n",
    "    # Create dataset\n",
    "    for form in inputDict['forms']:\n",
    "        d[form] = {}\n",
    "        for agg in inputDict['aggregates']:\n",
    "            d[form][agg] = {} \n",
    "\n",
    "            #create dataset based on values forms and aggregation methods\n",
    "            dataset = aggregate_tweets(tweets, agg, form)\n",
    "\n",
    "            # Add labels\n",
    "            for direction in inputDict['directions']:\n",
    "                d[form][agg][direction] = {}\n",
    "                for window in windows:\n",
    "                    d[form][agg][direction][window] = {}\n",
    "\n",
    "                    # get direction of window\n",
    "                    if direction == 'past':\n",
    "                        window_dir = window\n",
    "                    elif direction == 'future':\n",
    "                        window_dir = -1*window\n",
    "\n",
    "                    # Add label based on window to dataset\n",
    "                    labeled_dataset = get_label(dataset, window_dir)\n",
    "                    labeled_dataset = labeled_dataset.sample(frac=1) # shuffle\n",
    "\n",
    "                    text = labeled_dataset['text']\n",
    "                    label = labeled_dataset['Label']\n",
    "                    \n",
    "                    for vec in inputDict['vectorizers']:\n",
    "                        d[form][agg][direction][window][vec] = {}\n",
    "                        features = BOW_vectorize(text, vec)\n",
    "                        print(form +' '+ agg +' '+ direction +' '+ str(window) +' '+ vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eval(object):\n",
    "\n",
    "    def __init__(self, inputDict, tweets, prices):\n",
    "        self.tweets = tweets\n",
    "        self.prices = prices\n",
    "        self.inputs = inputDict\n",
    "    \n",
    "    def\n",
    "    \n",
    "    def load_marketdata(path, add_grid = True):\n",
    "        stockDF = pd.read_csv(path)\n",
    "        stockDF['DateTime'] = stockDF['Date'] + ' ' + stockDF['Time']\n",
    "        stockDF['DateTime'] = pd.to_datetime(stockDF['DateTime'])\n",
    "        stockDF = stockDF.set_index('DateTime')\n",
    "\n",
    "        if add_grid:\n",
    "            # Create grid\n",
    "            grid_start = min(stockDF.index) - pd.DateOffset(days=5)\n",
    "            grid_end = max(stockDF.index) + pd.DateOffset(days=5)\n",
    "            grid   = load_grid(start = grid_start, end = grid_end)\n",
    "\n",
    "            # Join grid with data\n",
    "            stockDF = grid.join(stockDF)\n",
    "            was_NaN = stockDF['Close'].isnull()\n",
    "            stockDF = stockDF.fillna(method = 'ffill')\n",
    "            stockDF['was_NaN'] = was_NaN\n",
    "\n",
    "        return stockDF\n",
    "        \n",
    "    def create_corpuses(self):\n",
    "        self.corpuses = {}\n",
    "        self.corpuses_list = []\n",
    "        \n",
    "        for form in self.inputs['forms']:\n",
    "            for agg in self.inputs['aggregates']:\n",
    "                corpus_id = (form, agg)\n",
    "                print ('Aggregating: '+ str(corpus_id) )\n",
    "                \n",
    "                self.corpuses[corpus_id] = aggregate_tweets(self.tweets, agg, form)\n",
    "                self.corpuses_list.append(corpus_id)\n",
    "                \n",
    "    def create_labels(self):\n",
    "        self.labels = {}\n",
    "        self.label_list = []\n",
    "        \n",
    "        for item in self.corpuses_list:\n",
    "            for direction in self.inputs['directions']:\n",
    "                for window in self.inputs['windows']:\n",
    "                    label_id = item + (direction, window)\n",
    "                    \n",
    "                    self.labeled_corpuses[label_id] = {}\n",
    "                    self.labeled_corpuses_list.append(labeled_corpus_id)\n",
    "                    \n",
    "                    # get direction of window\n",
    "                    if direction == 'past':\n",
    "                        window_dir = window\n",
    "                    elif direction == 'future':\n",
    "                        window_dir = -1*window\n",
    "\n",
    "                    # Add label based on window to dataset\n",
    "                    self.labels[corpus_id] = get_label(corpuses[item], prices,  window_dir)\n",
    "                    self.labels.append(corpus_id)\n",
    "                    #labeled_dataset = \n",
    "                    \n",
    "    def create_BOW_datasets(self):\n",
    "        self.datasets = {}\n",
    "        self.datasets_list = []\n",
    "        \n",
    "        # Iterate over corpuses\n",
    "        for item in self.corpuses_list:\n",
    "            for vec in inputDict['vectorizers']:\n",
    "                dataset_id = item + (vec,)\n",
    "                \n",
    "                # Vectorize text corpus\n",
    "                text = self.corpuses[item]['text']\n",
    "                self.datasets[dataset_id] = BOW_vectorize(text, vec)\n",
    "                self.datasets_list.append(dataset_id)\n",
    "                    \n",
    "                \n",
    "    def create_WV_datasets(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweetz = tweets[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 654 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f.create_BOW_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8corpuses by agregating\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = Eval(inputDict, tweetz)\n",
    "f.create_corpuses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = f.corpuses[('tokens', 'min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11x2391 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4767 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = BOW_vectorize(x['text'], 'tfidf')\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(textDF, pricesDF, shift, biclass = True):\n",
    "    \"\"\"\n",
    "    shift = n  - label is n minutes lagged\n",
    "    shift = -n  - label is n minute in future\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(pricesDF['Close'])\n",
    "    \n",
    "    if shift > 0 :\n",
    "        df['minLag'] = df['Close'].shift(shift)\n",
    "        conditions = [df['minLag'] == df['Close'], df['minLag'] < df['Close'], df['minLag'] > df['Close']]\n",
    "        df['Label'] = np.select(conditions, ['NoChange', 'Growth', 'Decline'], default='Missing')\n",
    "    else:\n",
    "        df['minShift'] = df['Close'].shift(shift)\n",
    "        conditions = [df['minShift'] == df['Close'], df['minShift'] > df['Close'], df['minShift'] < df['Close']]\n",
    "        df['Label'] = np.select(conditions, ['NoChange', 'Growth', 'Decline'], default='Missing')\n",
    "    \n",
    "    # delete missing label, and also nochange labels if biclass TRUE\n",
    "    df.loc[df['Label'] == 'Missing', 'Label'] = np.nan\n",
    "    if biclass:\n",
    "        df.loc[df['Label'] == 'NoChange', 'Label'] = np.nan\n",
    "        \n",
    "    text_index = pd.DataFrame(index = textDF.index)\n",
    "    labelDF = text_index.join(df)\n",
    "    labelDF = labelDF.reset_index()\n",
    "    \n",
    "    return labelDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = get_label(x, prices, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(393)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e.create_labeled_corpuses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lemmas', 'hour'),\n",
       " ('lemmas', '5min'),\n",
       " ('lemmas', 'min'),\n",
       " ('lemmas', 'none'),\n",
       " ('tokens', 'hour'),\n",
       " ('tokens', '5min'),\n",
       " ('tokens', 'min'),\n",
       " ('tokens', 'none')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.corpuses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens none future 1 log_tfidf_sw\n",
      "Wall time: 12min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d = BOW_gridsearch(inputDict)\n",
    "dataframe = reform_BOW_gridsearch(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>log_tfidf_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">tokens</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">none</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">future</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">L1_logit</th>\n",
       "      <th>acc</th>\n",
       "      <td>0.507453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>0.014786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">L2_logit</th>\n",
       "      <th>acc</th>\n",
       "      <td>0.510229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>0.020406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">nb</th>\n",
       "      <th>acc</th>\n",
       "      <td>0.510915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>0.021690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               0\n",
       "                                    log_tfidf_sw\n",
       "tokens none future 1 L1_logit acc       0.507453\n",
       "                              kappa     0.014786\n",
       "                     L2_logit acc       0.510229\n",
       "                              kappa     0.020406\n",
       "                     nb       acc       0.510915\n",
       "                              kappa     0.021690"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.unstack(level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature set 3: Word embeddings </h1>\n",
    "\n",
    "Word vector in documents are agreggated:\n",
    "1. Average\n",
    "2. Min\n",
    "3. Max\n",
    "4. Min + Max concacenation (2 times dimension of word vector)\n",
    "5. Weighted average with IDF as weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization methods\n",
    "def Tweet2Vec_mean(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            wordVec = embedding.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue   \n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "def Tweet2Vec_min(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = embedding.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.min(tweetVec, axis=0)\n",
    "\n",
    "def Tweet2Vec_max(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            wordVec = embedding.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.max(tweetVec, axis=0)\n",
    "\n",
    "def Tweet2Vec_minmax(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = embedding.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    minVec = np.min(tweetVec, axis=0)\n",
    "    maxVec = np.max(tweetVec, axis=0)\n",
    "    return np.append(maxVec, minVec)\n",
    "\n",
    "def Tweet2Vec_tfidf(tokens, embedding, weights):\n",
    "    tweetVec = []\n",
    "    weightSum = 0\n",
    "    \n",
    "    vocabulary = weights.vocabulary_\n",
    "    idf = weights.idf_\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = embedding.wv[word]\n",
    "            weight = idf[vocabulary[word]]\n",
    "            \n",
    "            weightSum = weightSum + weight\n",
    "            tweetVec.append(wordVec*weight)/weightSum\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import text as txt\n",
    "def Tweet2Vec_mean_sw(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            if word not in txt.ENGLISH_STOP_WORDS:\n",
    "                wordVec = embedding.wv[word]\n",
    "                tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "\n",
    "def embbeding_vectorize(inputText, embedding, method):\n",
    "    # Embedding simple average\n",
    "    if method == 'mean':\n",
    "        df = inputText.apply(Tweet2Vec_mean, args=[embedding])\n",
    "\n",
    "    # Embedding simple average without stopwords\n",
    "    elif method == 'mean_sw':\n",
    "        df = inputText.apply(Tweet2Vec_mean_sw, args=[embedding])\n",
    "\n",
    "    # Embedding minimum + maxiumum values concacenated\n",
    "    elif method == 'minmax':\n",
    "        df = inputText.apply(Tweet2Vec_minmax, args=[embedding])\n",
    "    \n",
    "    # Embedding IDF weighted average\n",
    "    elif method == 'idf':\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "        train = vec.fit_transform(inputText)\n",
    "        df = inputText.apply(Tweet2Vec_tfidf, args=[embedding, vec])\n",
    "    \n",
    "    # Embedding IDF weighted average without stopwords\n",
    "    elif method == 'idf_sw':\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, stop_words='english')\n",
    "        train = vec.fit_transform(inputText)\n",
    "        df = inputText.apply(Tweet2Vec_tfidf, args=[embedding, vec])\n",
    "    else:\n",
    "        raise ValueError('Method is not supported')\n",
    "        \n",
    "    train = df.apply(pd.Series).fillna(0)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load word Embedings\n",
    "# Load Glove (200D twitter trained)\n",
    "DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\glove.twitter.27B.200d_edited.txt'\n",
    "Twitter_200D = gensim.models.KeyedVectors.load_word2vec_format(DataPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load word Embedings\n",
    "# Load Glove (200D twitter trained)\n",
    "DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\glove.twitter.27B.200d_edited.txt'\n",
    "Twitter_200D = gensim.models.KeyedVectors.load_word2vec_format(DataPath)\n",
    "\n",
    "# Load W2V (200D Google News trained)\n",
    "DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\GoogleNews-vectors-negative300.bin'\n",
    "GoogleNews_300D = gensim.models.KeyedVectors.load_word2vec_format(DataPath, binary=True)\n",
    "\n",
    "## Load word Embedings\n",
    "# Load Glove (25D twitter trained)\n",
    "DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\glove.twitter.27B.25d.txt'\n",
    "Twitter_25D = gensim.models.KeyedVectors.load_word2vec_format(DataPath)\n",
    "\n",
    "## Load word Embedings\n",
    "# Load Glove (300D Wiki trained)\n",
    "DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\glove.840B.300d.txt'\n",
    "Wikipedia_300D = gensim.models.KeyedVectors.load_word2vec_format(DataPath)\n",
    "\n",
    "embedding_dict = {'Twitter_200D':Twitter_200D, 'GoogleNews_300D': GoogleNews_300D, \n",
    "               'Twitter_25D':Twitter_25D, 'Wikipedia_300D':Wikipedia_300D}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forms = ['lemmas', 'tokens']\n",
    "aggregates = ['hour', '5min', 'min', 'none']\n",
    "\n",
    "#labels\n",
    "directions = ['past', 'future']\n",
    "windows = [60, 1]\n",
    "\n",
    "embbedings = ['Twitter_200D', 'Twitter_25D', 'GoogleNews_300D', 'Wikipedia_300D']\n",
    "vectorizers = ['mean', 'mean_sw', 'minmax', 'idf', 'idf_sw']\n",
    "\n",
    "# validation\n",
    "models = ['logit', 'L2_logit']\n",
    "metrics = ['kappa', 'acc']\n",
    "\n",
    "inputDict = {'forms':forms, 'aggregates':aggregates, 'directions':directions, 'windows':windows,\n",
    "             'embbedings':embbedings, 'vectorizers':vectorizers, 'models':models, 'metrics':metrics}\n",
    "\n",
    "embedding_dict = {'Twitter_200D':Twitter_200D, 'GoogleNews_300D': GoogleNews_300D, \n",
    "                  'Twitter_25D':Twitter_25D, 'Wikipedia_300D':Wikipedia_300D}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = ['tokens']\n",
    "aggregates = ['none']\n",
    "\n",
    "#labels\n",
    "directions = ['future']\n",
    "windows = [1]\n",
    "\n",
    "embbedings = ['Twitter_200D']\n",
    "vectorizers = ['mean', 'mean_sw', 'minmax', 'idf', 'idf_sw']\n",
    "\n",
    "# validation\n",
    "models = ['logit', 'L2_logit']\n",
    "metrics = ['kappa', 'acc']\n",
    "\n",
    "inputDict = {'forms':forms, 'aggregates':aggregates, 'directions':directions, 'windows':windows,\n",
    "             'embbedings':embbedings, 'vectorizers':vectorizers, 'models':models, 'metrics':metrics}\n",
    "\n",
    "embedding_dict = {'Twitter_200D':Twitter_200D}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMB_gridsearch(inputDict):\n",
    "    d = {}\n",
    "    # Create dataset\n",
    "    for form in inputDict['forms']:\n",
    "        d[form] = {}\n",
    "        for agg in inputDict['aggregates']:\n",
    "            d[form][agg] = {} \n",
    "\n",
    "            #create dataset based on values forms and aggregation methods\n",
    "            dataset = aggregate_tweets(tweets, agg, form)\n",
    "\n",
    "            # Add labels\n",
    "            for direction in inputDict['directions']:\n",
    "                d[form][agg][direction] = {}\n",
    "                for window in windows:\n",
    "                    d[form][agg][direction][window] = {}\n",
    "\n",
    "                    # get direction of window\n",
    "                    if direction == 'past':\n",
    "                        window_dir = window\n",
    "                    elif direction == 'future':\n",
    "                        window_dir = -1*window\n",
    "\n",
    "                    # Add label based on window to dataset\n",
    "                    labeled_dataset = get_label(dataset, window_dir)\n",
    "                    labeled_dataset = labeled_dataset.sample(frac=1) # shuffle\n",
    "\n",
    "                    text = labeled_dataset['text']\n",
    "                    label = labeled_dataset['Label']\n",
    "\n",
    "                    # load embbeding\n",
    "                    for emb in inputDict['embbedings']:\n",
    "                        d[form][agg][direction][window][emb] = {}\n",
    "\n",
    "                        embedding = embedding_dict[emb]\n",
    "                        # create features using vectorizer\n",
    "                        for vec in inputDict['vectorizers']:\n",
    "                            d[form][agg][direction][window][emb][vec] = {}\n",
    "                            features = embbeding_vectorize(text, embedding, vec)\n",
    "                            print(form +' '+ agg +' '+ direction +' '+ str(window) +' '+ vec)\n",
    "\n",
    "                            # validate dataset using models and metrics\n",
    "                            for model in inputDict['models']:\n",
    "                                d[form][agg][direction][window][emb][vec][model] = {}\n",
    "                                pred = get_model_prediction(features, label, model)\n",
    "                                for metric in inputDict['metrics']:\n",
    "                                    value = get_metric(pred, label, metric)\n",
    "                                    d[form][agg][direction][window][emb][vec][model][metric] = value\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def reform_embedding(inputDict):\n",
    "    reform = {(level1_key, level2_key, level3_key, level4_key, level5_key, level6_key, level7_key,level8_key): values\n",
    "        for level1_key, level2_dict in inputDict.items()\n",
    "        for level2_key, level3_dict in level2_dict.items()\n",
    "        for level3_key, level4_dict in level3_dict.items()\n",
    "        for level4_key, level5_dict in level4_dict.items()\n",
    "        for level5_key, level6_dict in level5_dict.items()\n",
    "        for level6_key, level7_dict in level6_dict.items()\n",
    "        for level7_key, level8_dict in level7_dict.items()        \n",
    "        for level8_key, values      in level8_dict.items()}\n",
    "    dataFrame = pd.DataFrame(reform, index=[0]).T\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens none future 1 mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:73: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens none future 1 mean_sw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens none future 1 minmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:56: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens none future 1 idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:56: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens none future 1 idf_sw\n",
      "Wall time: 41min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d = EMB_gridsearch(inputDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>idf</th>\n",
       "      <th>idf_sw</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean_sw</th>\n",
       "      <th>minmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">tokens</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">none</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">future</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">Twitter_200D</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">L2_logit</th>\n",
       "      <th>acc</th>\n",
       "      <td>0.502697</td>\n",
       "      <td>0.501379</td>\n",
       "      <td>0.502715</td>\n",
       "      <td>0.502059</td>\n",
       "      <td>0.501327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>0.005167</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.002536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">logit</th>\n",
       "      <th>acc</th>\n",
       "      <td>0.502721</td>\n",
       "      <td>0.501350</td>\n",
       "      <td>0.502393</td>\n",
       "      <td>0.502024</td>\n",
       "      <td>0.501338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>0.005214</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.002560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0            \\\n",
       "                                                       idf    idf_sw   \n",
       "tokens none future 1 Twitter_200D L2_logit acc    0.502697  0.501379   \n",
       "                                           kappa  0.005167  0.002521   \n",
       "                                  logit    acc    0.502721  0.501350   \n",
       "                                           kappa  0.005214  0.002463   \n",
       "\n",
       "                                                                                \n",
       "                                                      mean   mean_sw    minmax  \n",
       "tokens none future 1 Twitter_200D L2_logit acc    0.502715  0.502059  0.501327  \n",
       "                                           kappa  0.005215  0.003904  0.002536  \n",
       "                                  logit    acc    0.502393  0.502024  0.501338  \n",
       "                                           kappa  0.004572  0.003836  0.002560  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = reform_embedding(d)\n",
    "x.unstack(level=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "a = {'hello': 'world'}\n",
    "\n",
    "with open('filename.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>idf</th>\n",
       "      <th>idf_sw</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean_sw</th>\n",
       "      <th>minmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">lemmas</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">hour</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">past</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">60</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">GoogleNews_300D</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">L2_logit</th>\n",
       "      <th>acc</th>\n",
       "      <td>0.510163</td>\n",
       "      <td>0.519512</td>\n",
       "      <td>0.511789</td>\n",
       "      <td>0.510976</td>\n",
       "      <td>0.482114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.038984</td>\n",
       "      <td>0.023376</td>\n",
       "      <td>0.021804</td>\n",
       "      <td>-0.035807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">logit</th>\n",
       "      <th>acc</th>\n",
       "      <td>0.496748</td>\n",
       "      <td>0.496748</td>\n",
       "      <td>0.491870</td>\n",
       "      <td>0.500407</td>\n",
       "      <td>0.481301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>-0.006526</td>\n",
       "      <td>-0.006554</td>\n",
       "      <td>-0.016311</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>-0.037392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Twitter_200D</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">L2_logit</th>\n",
       "      <th>acc</th>\n",
       "      <td>0.488618</td>\n",
       "      <td>0.481707</td>\n",
       "      <td>0.506098</td>\n",
       "      <td>0.515854</td>\n",
       "      <td>0.486179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>-0.022774</td>\n",
       "      <td>-0.036557</td>\n",
       "      <td>0.012109</td>\n",
       "      <td>0.031677</td>\n",
       "      <td>-0.027681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">logit</th>\n",
       "      <th>acc</th>\n",
       "      <td>0.484959</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.495528</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.491463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kappa</th>\n",
       "      <td>-0.030057</td>\n",
       "      <td>-0.033296</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>-0.017142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0            \\\n",
       "                                                         idf    idf_sw   \n",
       "lemmas hour past 60 GoogleNews_300D L2_logit acc    0.510163  0.519512   \n",
       "                                             kappa  0.020310  0.038984   \n",
       "                                    logit    acc    0.496748  0.496748   \n",
       "                                             kappa -0.006526 -0.006554   \n",
       "                    Twitter_200D    L2_logit acc    0.488618  0.481707   \n",
       "                                             kappa -0.022774 -0.036557   \n",
       "                                    logit    acc    0.484959  0.483333   \n",
       "                                             kappa -0.030057 -0.033296   \n",
       "\n",
       "                                                                        \\\n",
       "                                                        mean   mean_sw   \n",
       "lemmas hour past 60 GoogleNews_300D L2_logit acc    0.511789  0.510976   \n",
       "                                             kappa  0.023376  0.021804   \n",
       "                                    logit    acc    0.491870  0.500407   \n",
       "                                             kappa -0.016311  0.000785   \n",
       "                    Twitter_200D    L2_logit acc    0.506098  0.515854   \n",
       "                                             kappa  0.012109  0.031677   \n",
       "                                    logit    acc    0.495528  0.495935   \n",
       "                                             kappa -0.009007 -0.008116   \n",
       "\n",
       "                                                              \n",
       "                                                      minmax  \n",
       "lemmas hour past 60 GoogleNews_300D L2_logit acc    0.482114  \n",
       "                                             kappa -0.035807  \n",
       "                                    logit    acc    0.481301  \n",
       "                                             kappa -0.037392  \n",
       "                    Twitter_200D    L2_logit acc    0.486179  \n",
       "                                             kappa -0.027681  \n",
       "                                    logit    acc    0.491463  \n",
       "                                             kappa -0.017142  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = reform_embedding(d)\n",
    "x.unstack(level=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alternative version of embeding averaging\n",
    "\n",
    "def tweet2vec_tfidf(tokens, embedding, tfidf):\n",
    "    tweetVec = []\n",
    "    weights = []\n",
    "    \n",
    "    vocabulary = tfidf.vocabulary_\n",
    "    idf = tfidf.idf_\n",
    "    \n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = np.array(embedding[word])\n",
    "            weight = idf[vocabulary[word]]\n",
    "            \n",
    "            tweetVec.append(wordVec)\n",
    "            weights.append(weight)\n",
    "        except: continue\n",
    "            \n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(embedding.vector_size)\n",
    "        return tweetVec\n",
    "        \n",
    "    weights = weights / np.sum(weights)\n",
    "    tweetVec = np.array(tweetVec)\n",
    "    weighted_vec = tweetVec * weights[:,None]\n",
    "    return weighted_vec.sum(axis = 0)\n",
    "\n",
    "def tweet2vec_tfidf2(tokens, embedding, tfidf):\n",
    "    tweetVec = []\n",
    "    weights = 0\n",
    "    \n",
    "    vocabulary = tfidf.vocabulary_\n",
    "    idf = tfidf.idf_\n",
    "    \n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = np.array(embedding[word])\n",
    "            weight = idf[vocabulary[word]]\n",
    "            \n",
    "            weights = weights + weight\n",
    "            tweetVec.append(wordVec*weight)\n",
    "        except: continue\n",
    "            \n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(embedding.vector_size)\n",
    "        return tweetVec\n",
    "    \n",
    "    weighted_vec = tweetVec / weights\n",
    "    return weighted_vec.sum(axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
