{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Standard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# word embedings\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Vectorization and evaluation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tweets(inputDF, freq, forms):\n",
    "    '''\n",
    "    Agregates text over selected frequency.\n",
    "\n",
    "    Selectable frequencies are 'hour', '5min' ,'minute' and 'none' for no aggragating (whole tweets are returned)\n",
    "    Tweets with identical text occuring more than once per day are assumed to be spamm and are filtered.\n",
    "\n",
    "    '''\n",
    "    tweets = inputDF.copy()\n",
    "    special = ['F_exclamation', 'F_question', 'F_ellipsis', 'F_hashtags', 'F_cashtags', 'F_usermention', 'F_urls']\n",
    "\n",
    "    if freq == 'none':\n",
    "        level = ['date', 'hour', '5min', 'minute', 'id']\n",
    "    elif freq == 'min':\n",
    "        level = ['date', 'hour', '5min', 'minute']\n",
    "    elif freq == '5min':\n",
    "        level = ['date', 'hour', '5min']\n",
    "    elif freq == 'hour':\n",
    "        level = ['date', 'hour']\n",
    "    else:\n",
    "        print('Unsupported frequency') \n",
    "        return\n",
    "\n",
    "    # Aggregate tweets and special features\n",
    "    sum_text = tweets[forms].groupby(level=level).apply(sum).rename(\"text\")\n",
    "    sum_special = tweets[special].groupby(level=level).sum().add_prefix('sum')\n",
    "    avg_special = tweets[special].groupby(level=level).mean().add_prefix('avg')\n",
    "    count_tweets = tweets.groupby(level=level).size().rename('tweet_count')\n",
    "    df = pd.concat([sum_special, avg_special, count_tweets, sum_text], axis = 1)\n",
    "\n",
    "    # Reconstruct index to single lablel\n",
    "    df = df.reset_index()\n",
    "    if freq == 'none':\n",
    "        df['DateTime'] = df['date'] + ' ' + df['hour'].astype(str) + ':' + df['minute'].astype(str)\n",
    "        df = df.drop(['date', 'hour', '5min', 'minute', 'id'], axis=1)\n",
    "    elif freq == 'min':\n",
    "        df['DateTime'] = df['date'] + ' ' + df['hour'].astype(str) + ':' + df['minute'].astype(str)\n",
    "        df = df.drop(['date', 'hour', '5min', 'minute'], axis=1)\n",
    "    elif freq == '5min':\n",
    "        df['DateTime'] = df['date'] + ' ' + df['hour'].astype(str) + ':' + df['5min'].astype(str)\n",
    "        df = df.drop(['date', 'hour', '5min'], axis=1)\n",
    "    elif freq == 'hour':\n",
    "        df['DateTime'] = df['date'] + ' ' + df['hour'].astype(str)\n",
    "        df = df.drop(['date', 'hour'], axis=1)\n",
    "    else: return\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])    \n",
    "    df = df.set_index('DateTime')\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_label(textDF, pricesDF, shift):\n",
    "    \"\"\"\n",
    "    shift = n  - label is n minutes lagged\n",
    "    shift = -n  - label is n minute in future\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(pricesDF['Close'])\n",
    "    \n",
    "    if shift > 0 :\n",
    "        df['minLag'] = df['Close'].shift(shift)\n",
    "        conditions = [df['minLag'] == df['Close'], df['minLag'] < df['Close'], df['minLag'] > df['Close']]\n",
    "        df['Label'] = np.select(conditions, ['NoChange', 'Growth', 'Decline'], default='Missing')\n",
    "    else:\n",
    "        df['minShift'] = df['Close'].shift(shift)\n",
    "        conditions = [df['minShift'] == df['Close'], df['minShift'] > df['Close'], df['minShift'] < df['Close']]\n",
    "        df['Label'] = np.select(conditions, ['NoChange', 'Growth', 'Decline'], default='Missing')\n",
    "    \n",
    "    # delete missing label, and also nochange labels if biclass TRUE\n",
    "    df.loc[df['Label'] == 'Missing', 'Label'] = np.nan\n",
    "    df.loc[df['Label'] == 'NoChange', 'Label'] = np.nan\n",
    "        \n",
    "    text_index = pd.DataFrame(index = textDF.index)\n",
    "    labelDF = text_index.join(df)\n",
    "    labelDF = labelDF.reset_index()\n",
    "    \n",
    "    return labelDF\n",
    "\n",
    "def BOW_vectorize(inputText, method):\n",
    "    '''\n",
    "    Calls scikit text vectorizers based on parameters. Returns sparse matrix. \n",
    "\n",
    "    '''\n",
    "    # binary terms vectorizer\n",
    "    if method == 'binary':\n",
    "        vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, binary=True)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    # Simple count vectorizer\n",
    "    elif method == 'count':\n",
    "        vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, binary=False)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    # Simple count vectorizer with stopwords filter\n",
    "    elif method == 'count_sw':\n",
    "        vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, stop_words='english', binary=False)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    # Term frequencies vectorizer\n",
    "    elif method =='frequency':\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, sublinear_tf = False, use_idf=False)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    #simple TFIDF vectorizer\n",
    "    elif method =='tfidf':\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, sublinear_tf = False, use_idf=True)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    elif method =='tfidf_sw':\n",
    "        #simple TFIDF vectorizer with english stop words\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, stop_words='english',sublinear_tf = False, use_idf=True)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    elif method =='log_tfidf':\n",
    "        #LOG tf TFIDF vectorizer\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, sublinear_tf = True, use_idf=True)\n",
    "        train = vec.fit_transform(inputText)\n",
    "\n",
    "    elif method =='log_tfidf_sw':\n",
    "        #LOG tf TFIDF vectorizer with english stop words\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, stop_words='english', sublinear_tf = True, use_idf=True)\n",
    "        train = vec.fit_transform(inputText)\n",
    "    else:\n",
    "        raise ValueError('Method is not supported')\n",
    "    return train\n",
    "\n",
    "# Vectorization methods\n",
    "def tweet2vec_mean(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            wordVec = embedding.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue   \n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "def tweet2vec_minmax(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = embedding.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    minVec = np.min(tweetVec, axis=0)\n",
    "    maxVec = np.max(tweetVec, axis=0)\n",
    "    return np.append(maxVec, minVec)\n",
    "\n",
    "def tweet2vec_tfidf(tokens, embedding, weights):\n",
    "    tweetVec = []\n",
    "    weightSum = 0\n",
    "    \n",
    "    vocabulary = weights.vocabulary_\n",
    "    idf = weights.idf_\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = embedding.wv[word]\n",
    "            weight = idf[vocabulary[word]]\n",
    "            \n",
    "            weightSum = weightSum + weight\n",
    "            tweetVec.append(wordVec*weight)/weightSum\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import text as txt\n",
    "def tweet2vec_mean_sw(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            if word not in txt.ENGLISH_STOP_WORDS:\n",
    "                wordVec = embedding.wv[word]\n",
    "                tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "\n",
    "def VW_vectorize(inputText, embedding, method):\n",
    "    # Embedding simple average\n",
    "    if method == 'mean':\n",
    "        df = inputText.apply(tweet2vec_mean, args=[embedding])\n",
    "\n",
    "    # Embedding simple average without stopwords\n",
    "    elif method == 'mean_sw':\n",
    "        df = inputText.apply(tweet2vec_mean_sw, args=[embedding])\n",
    "\n",
    "    # Embedding minimum + maxiumum values concacenated\n",
    "    elif method == 'minmax':\n",
    "        df = inputText.apply(tweet2vec_minmax, args=[embedding])\n",
    "    \n",
    "    # Embedding IDF weighted average\n",
    "    elif method == 'idf':\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "        train = vec.fit_transform(inputText)\n",
    "        df = inputText.apply(tweet2vec_tfidf, args=[embedding, vec])\n",
    "    \n",
    "    # Embedding IDF weighted average without stopwords\n",
    "    elif method == 'idf_sw':\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, stop_words='english')\n",
    "        train = vec.fit_transform(inputText)\n",
    "        df = inputText.apply(tweet2vec_tfidf, args=[embedding, vec])\n",
    "    else:\n",
    "        raise ValueError('Method is not supported')\n",
    "        \n",
    "    train = df.apply(pd.Series).fillna(0)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features(object):\n",
    "\n",
    "    def __init__(self, inputDict):\n",
    "        self.inputs = inputDict\n",
    "        self.price_path = 'AAPL_1min.csv'\n",
    "        self.tweets_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataProcessed\\\\tweetsAAPL.csv'\n",
    "\n",
    "    def load_data(self):\n",
    "        self.tweets = Features.load_tweets(self.tweets_path)\n",
    "        self.prices = Features.load_prices(self.price_path, add_grid = True)\n",
    "        \n",
    "    def load_embeddings(self):\n",
    "        Twitter_200D_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\glove.twitter.27B.200d.txt'\n",
    "        Twitter_200D = gensim.models.KeyedVectors.load_word2vec_format(Twitter_200D_path)\n",
    "        \n",
    "        GoogleNews_300D_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\GoogleNews-vectors-negative300.bin'\n",
    "        GoogleNews_300D = gensim.models.KeyedVectors.load_word2vec_format(GoogleNews_300D_path, binary=True)\n",
    "        \n",
    "        Wikipedia_300D_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\glove.840B.300d.txt'\n",
    "        Wikipedia_300D = gensim.models.KeyedVectors.load_word2vec_format(Wikipedia_300D_path)\n",
    "        \n",
    "        self.embeddings = {'Twitter_200D':Twitter_200D, 'GoogleNews_300D': GoogleNews_300D, 'Wikipedia_300D':Wikipedia_300D}\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_prices(path, add_grid = True):\n",
    "        '''\n",
    "        Loads prices from csv file.\n",
    "        \n",
    "        Returns dataframe with datetime index. Original prices from csv are placed on datetime grid\n",
    "        with one minute frequency over oldest and newest price observations. This is done include After-Hours\n",
    "        price changes - missing prices created by the grid are frontfilled by last valid observations.\n",
    "        \n",
    "        '''\n",
    "        prices = pd.read_csv(path)\n",
    "        prices['DateTime'] = prices['Date'] + ' ' + prices['Time']\n",
    "        prices['DateTime'] = pd.to_datetime(prices['DateTime'])\n",
    "        prices = prices.drop(['Date', 'Time', 'Volume'], axis=1)\n",
    "        prices = prices.set_index('DateTime')\n",
    "                     \n",
    "        if add_grid:\n",
    "            # Create grid\n",
    "            grid_start = min(prices.index) - pd.DateOffset(days=5)\n",
    "            grid_end = max(prices.index) + pd.DateOffset(days=5)\n",
    "            grid = pd.date_range(start=grid_start, end=grid_end, freq='min')\n",
    "            grid = pd.Series(grid).rename('DateTime')\n",
    "            grid = pd.DataFrame(grid).set_index('DateTime')\n",
    "\n",
    "            # Join grid with data\n",
    "            prices = grid.join(prices)\n",
    "            was_NaN = prices['Close'].isnull()\n",
    "            prices = prices.fillna(method = 'ffill')\n",
    "            prices['was_NaN'] = was_NaN\n",
    "        return prices\n",
    "    \n",
    "    @staticmethod    \n",
    "    def load_tweets(path):\n",
    "        '''\n",
    "        Loads preprocessed tweets from csv file.\n",
    "        \n",
    "        Returns multiindexed data frame with 'date', 'hour', '5min' ,'minute', 'id' index levels.\n",
    "        Tweets with identical text occuring more than once per day are assumed to be spamm and are filtered.\n",
    "        \n",
    "        '''\n",
    "        # Load data from csv and convert column lists of words\n",
    "        tweets = pd.read_csv(path)\n",
    "        tweets['lemmas'] = tweets['lemmas'].apply(literal_eval)\n",
    "        tweets['tokens'] = tweets['tokens'].apply(literal_eval)\n",
    "\n",
    "        # Create time variables\n",
    "        tweets['date'] = tweets['created_at'].str[:10]\n",
    "        tweets['hour'] = tweets['created_at'].str[11:13]\n",
    "        tweets['minute'] = tweets['created_at'].str[14:16]\n",
    "        tweets['5min'] = (tweets['minute'].astype(int)//5)*5\n",
    "        \n",
    "        # Spam filtering - Remove duplicate tweets in date\n",
    "        tweets = tweets.drop_duplicates(['date', 'text'])\n",
    "       \n",
    "        # Drop redundant columns and index\n",
    "        tweets = tweets.drop(['Unnamed: 0', 'created_at', 'text'], axis=1)\n",
    "        tweets.set_index(['date', 'hour', '5min' ,'minute', 'id'], inplace = True)\n",
    "        return tweets\n",
    "\n",
    "    \n",
    "    def create_corpuses(self):\n",
    "        self.corpus = {}\n",
    "        self.corpus_list = []\n",
    "        \n",
    "        for form in self.inputs['forms']:\n",
    "            for agg in self.inputs['aggregates']:\n",
    "                corpus_id = (form, agg)\n",
    "                self.corpus_list.append(corpus_id)\n",
    "                \n",
    "                print ('Aggregating: '+ str(corpus_id))\n",
    "                self.corpus[corpus_id] = aggregate_tweets(self.tweets, agg, form)\n",
    "                \n",
    "                \n",
    "    def create_labels(self):\n",
    "        self.label = {}\n",
    "        self.label_list = []\n",
    "        \n",
    "        # Create list of label types\n",
    "        self.label_type_list = []\n",
    "        for direction in self.inputs['directions']:\n",
    "            for window in self.inputs['windows']:\n",
    "                label_type = (direction, window)\n",
    "                self.label_type_list.append(label_type)        \n",
    "        \n",
    "        # Iterate over corpuses and label types\n",
    "        for item in self.corpus_list:\n",
    "            for label_type in self.label_type_list:\n",
    "                label_id = item + label_type\n",
    "                self.label_list.append(label_id)\n",
    "\n",
    "                # Get direction of shift\n",
    "                direction = label_type[0]\n",
    "                window = label_type[1]                \n",
    "                if direction == 'past':\n",
    "                    window_dir = window\n",
    "                elif direction == 'future':\n",
    "                    window_dir = -1*window\n",
    "\n",
    "                # Add label based on window to dataset\n",
    "                self.label[label_id] = get_label(self.corpus[item], self.prices,  window_dir)\n",
    "                    \n",
    "    def create_BOW(self):\n",
    "        self.dataset = {}\n",
    "        self.dataset_list = []\n",
    "        \n",
    "        # Iterate over corpuses\n",
    "        for item in self.corpus_list:\n",
    "            for vec in inputDict['vectorizers']:\n",
    "                dataset_id = item + (vec,)\n",
    "                self.dataset_list.append(dataset_id)\n",
    "                \n",
    "                # Vectorize text corpus\n",
    "                text = self.corpus[item]['text']\n",
    "                self.dataset[dataset_id] = BOW_vectorize(text, vec)\n",
    "\n",
    "    def create_links(self):\n",
    "        self.link = {}\n",
    "        self.link_list = []\n",
    "\n",
    "        # Iterate over corpuses and label types\n",
    "        for item in self.dataset_list:\n",
    "            for label_type in self.label_type_list:\n",
    "                link_id = item + label_type\n",
    "                self.link_list.append(link_id)\n",
    "\n",
    "                # Search for suitable label in self.label\n",
    "                current_label_id = (item[0], item[1]) + label_type\n",
    "                current_label = self.label[current_label_id]\n",
    "\n",
    "                # Get array of indexes without NaN values\n",
    "                index = current_label[current_label['Label'].notnull()].index\n",
    "                self.link[link_id] = {'index': index, 'dataset_id': item, 'label_id': current_label_id}\n",
    "      \n",
    "    def create_WV(self):\n",
    "        return np.random.rand()\n",
    "    \n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.predictions = {}\n",
    "        self.results = {}\n",
    "        \n",
    "        # Iterate over dataset - label pairs\n",
    "        for item in self.link_list:\n",
    "            link = self.link[('lemmas', '5min', 'binary', 'future', 1)]\n",
    "            \n",
    "            # Extract dataset - label pair using links and shuffle \n",
    "            index = link['index']\n",
    "            index = np.random.permutation(index)\n",
    "            dataset = self.dataset[link['dataset_id']][index]\n",
    "            label = self.label[link['label_id']].reindex(index)['Label']\n",
    "            \n",
    "            # Iterate over models\n",
    "            for model in  inputDict['models']:\n",
    "                \n",
    "                # Calculate model predicitons\n",
    "                prediction = get_model_prediction(dataset, label, model)\n",
    "                prediction_id = item + (model,)\n",
    "                self.predictions[prediction_id] = prediction\n",
    "                \n",
    "                # Calculate accuracy and kappa metrics\n",
    "                kappa = cohen_kappa_score(label, prediction)\n",
    "                accuracy = accuracy_score(label, prediction)\n",
    "                \n",
    "                result_id_kappa = item + (model, 'kappa')\n",
    "                result_id_accuracy = item + (model, 'accuracy')\n",
    "                \n",
    "                self.results[result_id_kappa] = kappa\n",
    "                self.results[result_id_accuracy] = accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = ['lemmas', 'tokens']\n",
    "aggregates = ['hour', '5min', 'min', 'none']\n",
    "\n",
    "#labels\n",
    "directions = ['past', 'future']\n",
    "windows = [60, 1]\n",
    "\n",
    "vectorizers = ['binary', 'count', 'count_sw', 'frequency', 'tfidf', 'tfidf_sw', 'log_tfidf', 'log_tfidf_sw']\n",
    "\n",
    "# validation\n",
    "models = ['L2_logit', 'L1_logit', 'nb']\n",
    "metrics = ['kappa', 'acc']\n",
    "\n",
    "inputDict = {'forms':forms, 'aggregates':aggregates, 'directions':directions, \n",
    "             'windows':windows, 'vectorizers':vectorizers, 'models':models, 'metrics':metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = Features(inputDict)\n",
    "f.price_path = 'AAPL_1min.csv'\n",
    "f.tweets_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataProcessed\\\\tweetsAAPL.csv'\n",
    "\n",
    "f.load_data()\n",
    "f.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f.create_corpuses()\n",
    "f.create_BOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.create_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.create_links()\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vectorization methods\n",
    "def tweet2vec_mean(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            wordVec = embedding.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue   \n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "def tweet2vec_minmax(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = embedding.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    minVec = np.min(tweetVec, axis=0)\n",
    "    maxVec = np.max(tweetVec, axis=0)\n",
    "    return np.append(maxVec, minVec)\n",
    "\n",
    "def tweet2vec_tfidf(tokens, embedding, weights):\n",
    "    tweetVec = []\n",
    "    weightSum = 0\n",
    "    \n",
    "    vocabulary = weights.vocabulary_\n",
    "    idf = weights.idf_\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = embedding.wv[word]\n",
    "            weight = idf[vocabulary[word]]\n",
    "            \n",
    "            weightSum = weightSum + weight\n",
    "            tweetVec.append(wordVec*weight)/weightSum\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import text as txt\n",
    "def tweet2vec_mean_sw(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            if word not in txt.ENGLISH_STOP_WORDS:\n",
    "                wordVec = embedding.wv[word]\n",
    "                tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(1)\n",
    "    return np.mean(tweetVec, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating: ('lemmas', 'hour')\n",
      "Aggregating: ('lemmas', '5min')\n",
      "Aggregating: ('lemmas', 'min')\n",
      "Aggregating: ('lemmas', 'none')\n",
      "Aggregating: ('tokens', 'hour')\n",
      "Aggregating: ('tokens', '5min')\n",
      "Aggregating: ('tokens', 'min')\n",
      "Aggregating: ('tokens', 'none')\n"
     ]
    }
   ],
   "source": [
    "e = Features(inputDict)\n",
    "e.tweets = f.tweets[:1000]\n",
    "e.prices = f.prices\n",
    "e.embeddings = f.embeddings\n",
    "\n",
    "\n",
    "e.create_corpuses()\n",
    "e.create_BOW()\n",
    "e.create_labels()\n",
    "e.create_links()\n",
    "e.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(inputDF, labeling,  method, validations=5):\n",
    "    if method == 'logit':\n",
    "        model = LogisticRegression(C=1e30,penalty='l2')\n",
    "        pred = cross_val_predict(model, inputDF, labeling, cv=validations, n_jobs=1, verbose=0)\n",
    "        \n",
    "    elif method == 'L2_logit':\n",
    "        model = LogisticRegression(C=1, penalty='l2')\n",
    "        pred = cross_val_predict(model, inputDF, labeling, cv=validations, n_jobs=1, verbose=0)    \n",
    "        \n",
    "    elif method == 'L1_logit':\n",
    "        model = LogisticRegression(C=1, penalty='l1')\n",
    "        pred = cross_val_predict(model, inputDF, labeling, cv=validations, n_jobs=1, verbose=0)    \n",
    "        \n",
    "    elif method == 'nb':\n",
    "        model = MultinomialNB()\n",
    "        pred = cross_val_predict(model, inputDF, labeling, cv=validations, n_jobs=1, verbose=0)  \n",
    "    else:\n",
    "        raise ValueError('Method is not supported')\n",
    "        \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = e.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(list(x.items()))\n",
    "index = pd.MultiIndex.from_tuples(y[0])\n",
    "y = y.drop(0, axis = 1)\n",
    "y = y.rename(columns = {1:'results'})\n",
    "y = y.set_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results    (lemmas, none, tfidf_sw, past, 60, nb)\n",
       "dtype: object"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.xs('kappa', level=-1).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">results</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>binary</th>\n",
       "      <th>count</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>frequency</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>log_tfidf_sw</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tfidf_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">lemmas</th>\n",
       "      <th rowspan=\"12\" valign=\"top\">5min</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">future</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.041339</td>\n",
       "      <td>-0.036433</td>\n",
       "      <td>-0.217782</td>\n",
       "      <td>-0.189268</td>\n",
       "      <td>-0.132663</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>-0.086614</td>\n",
       "      <td>-0.093069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.079920</td>\n",
       "      <td>-0.170254</td>\n",
       "      <td>-0.198397</td>\n",
       "      <td>-0.186047</td>\n",
       "      <td>-0.178894</td>\n",
       "      <td>-0.191235</td>\n",
       "      <td>-0.073022</td>\n",
       "      <td>-0.203356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.144724</td>\n",
       "      <td>-0.105986</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.014028</td>\n",
       "      <td>0.160243</td>\n",
       "      <td>0.063555</td>\n",
       "      <td>-0.040201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">60</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>0.003937</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.151129</td>\n",
       "      <td>-0.290581</td>\n",
       "      <td>-0.232558</td>\n",
       "      <td>-0.073930</td>\n",
       "      <td>-0.029183</td>\n",
       "      <td>-0.119166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.079920</td>\n",
       "      <td>-0.086432</td>\n",
       "      <td>-0.099602</td>\n",
       "      <td>-0.320792</td>\n",
       "      <td>-0.171828</td>\n",
       "      <td>-0.237052</td>\n",
       "      <td>-0.093069</td>\n",
       "      <td>-0.225126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>-0.040201</td>\n",
       "      <td>0.057942</td>\n",
       "      <td>0.052261</td>\n",
       "      <td>-0.014028</td>\n",
       "      <td>-0.053785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043564</td>\n",
       "      <td>-0.014028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">past</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.229703</td>\n",
       "      <td>-0.329858</td>\n",
       "      <td>-0.215264</td>\n",
       "      <td>0.074583</td>\n",
       "      <td>-0.198397</td>\n",
       "      <td>-0.196271</td>\n",
       "      <td>0.023692</td>\n",
       "      <td>-0.157947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.267717</td>\n",
       "      <td>-0.271357</td>\n",
       "      <td>-0.279024</td>\n",
       "      <td>-0.237052</td>\n",
       "      <td>-0.159274</td>\n",
       "      <td>-0.145418</td>\n",
       "      <td>-0.066532</td>\n",
       "      <td>-0.191235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.046512</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.014028</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>-0.021718</td>\n",
       "      <td>0.032064</td>\n",
       "      <td>0.043564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">60</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.053785</td>\n",
       "      <td>-0.007968</td>\n",
       "      <td>0.029441</td>\n",
       "      <td>0.099804</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.137730</td>\n",
       "      <td>-0.007968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.099602</td>\n",
       "      <td>-0.186047</td>\n",
       "      <td>-0.093023</td>\n",
       "      <td>-0.067127</td>\n",
       "      <td>-0.152305</td>\n",
       "      <td>-0.086432</td>\n",
       "      <td>-0.164846</td>\n",
       "      <td>-0.020161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>-0.014028</td>\n",
       "      <td>-0.053785</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>0.108163</td>\n",
       "      <td>-0.093069</td>\n",
       "      <td>0.072581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">hour</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">future</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.067127</td>\n",
       "      <td>-0.079920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.138614</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>-0.151129</td>\n",
       "      <td>-0.177165</td>\n",
       "      <td>-0.112537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.125874</td>\n",
       "      <td>-0.125874</td>\n",
       "      <td>-0.125874</td>\n",
       "      <td>-0.132663</td>\n",
       "      <td>-0.225126</td>\n",
       "      <td>-0.164846</td>\n",
       "      <td>-0.237052</td>\n",
       "      <td>-0.106212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.109235</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>0.032064</td>\n",
       "      <td>-0.132663</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.086614</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>0.083665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">60</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.125245</td>\n",
       "      <td>-0.145418</td>\n",
       "      <td>-0.067127</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.138614</td>\n",
       "      <td>-0.137730</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>-0.099602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.177165</td>\n",
       "      <td>-0.066532</td>\n",
       "      <td>-0.157947</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.232558</td>\n",
       "      <td>-0.171828</td>\n",
       "      <td>-0.112537</td>\n",
       "      <td>-0.177165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.052261</td>\n",
       "      <td>0.052261</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>0.049213</td>\n",
       "      <td>0.078156</td>\n",
       "      <td>0.026210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">past</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.015702</td>\n",
       "      <td>-0.041339</td>\n",
       "      <td>-0.222441</td>\n",
       "      <td>-0.119675</td>\n",
       "      <td>0.017875</td>\n",
       "      <td>-0.047525</td>\n",
       "      <td>-0.067127</td>\n",
       "      <td>0.165323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.164846</td>\n",
       "      <td>-0.171828</td>\n",
       "      <td>-0.145418</td>\n",
       "      <td>-0.248766</td>\n",
       "      <td>-0.159274</td>\n",
       "      <td>-0.256207</td>\n",
       "      <td>-0.138614</td>\n",
       "      <td>-0.171828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.063555</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.063555</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.149850</td>\n",
       "      <td>-0.026369</td>\n",
       "      <td>-0.073486</td>\n",
       "      <td>-0.014028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">60</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.099602</td>\n",
       "      <td>-0.099602</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>-0.163424</td>\n",
       "      <td>-0.086432</td>\n",
       "      <td>-0.105986</td>\n",
       "      <td>-0.093023</td>\n",
       "      <td>0.017875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.106212</td>\n",
       "      <td>-0.244489</td>\n",
       "      <td>-0.099602</td>\n",
       "      <td>-0.067127</td>\n",
       "      <td>-0.099695</td>\n",
       "      <td>-0.093023</td>\n",
       "      <td>-0.014028</td>\n",
       "      <td>-0.132663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.052261</td>\n",
       "      <td>-0.014028</td>\n",
       "      <td>0.118952</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.081883</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>-0.053785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">min</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">future</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.112537</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>-0.164846</td>\n",
       "      <td>-0.093069</td>\n",
       "      <td>-0.093069</td>\n",
       "      <td>-0.106212</td>\n",
       "      <td>-0.177165</td>\n",
       "      <td>0.017875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.244489</td>\n",
       "      <td>-0.125245</td>\n",
       "      <td>-0.079920</td>\n",
       "      <td>-0.093069</td>\n",
       "      <td>-0.191235</td>\n",
       "      <td>-0.106212</td>\n",
       "      <td>-0.196271</td>\n",
       "      <td>-0.159274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.032064</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.047525</td>\n",
       "      <td>-0.026369</td>\n",
       "      <td>-0.007968</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.027805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">60</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.040201</td>\n",
       "      <td>-0.203356</td>\n",
       "      <td>-0.020161</td>\n",
       "      <td>-0.226964</td>\n",
       "      <td>0.023692</td>\n",
       "      <td>-0.099602</td>\n",
       "      <td>-0.131890</td>\n",
       "      <td>-0.073930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.026369</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.263736</td>\n",
       "      <td>-0.186047</td>\n",
       "      <td>-0.079920</td>\n",
       "      <td>-0.119166</td>\n",
       "      <td>-0.053785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>-0.027805</td>\n",
       "      <td>0.134653</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>-0.086614</td>\n",
       "      <td>0.057942</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>0.072581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">tokens</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">hour</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">past</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.217782</td>\n",
       "      <td>0.083665</td>\n",
       "      <td>-0.194231</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.151129</td>\n",
       "      <td>-0.014028</td>\n",
       "      <td>-0.119166</td>\n",
       "      <td>-0.125874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.066532</td>\n",
       "      <td>-0.139535</td>\n",
       "      <td>-0.041339</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.191235</td>\n",
       "      <td>-0.212982</td>\n",
       "      <td>-0.212982</td>\n",
       "      <td>-0.145418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>-0.112903</td>\n",
       "      <td>0.037849</td>\n",
       "      <td>0.063555</td>\n",
       "      <td>0.129482</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.020161</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.053785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">60</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.157947</td>\n",
       "      <td>-0.079920</td>\n",
       "      <td>-0.301887</td>\n",
       "      <td>-0.132663</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>-0.137730</td>\n",
       "      <td>-0.132663</td>\n",
       "      <td>0.023692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.152305</td>\n",
       "      <td>-0.060120</td>\n",
       "      <td>-0.093023</td>\n",
       "      <td>-0.171828</td>\n",
       "      <td>-0.093023</td>\n",
       "      <td>-0.106212</td>\n",
       "      <td>-0.145418</td>\n",
       "      <td>-0.119166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>-0.007968</td>\n",
       "      <td>-0.020161</td>\n",
       "      <td>-0.007968</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>0.052261</td>\n",
       "      <td>0.118952</td>\n",
       "      <td>-0.079920</td>\n",
       "      <td>0.083665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">min</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">future</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.001980</td>\n",
       "      <td>-0.125874</td>\n",
       "      <td>-0.177165</td>\n",
       "      <td>-0.131890</td>\n",
       "      <td>-0.093069</td>\n",
       "      <td>-0.093069</td>\n",
       "      <td>-0.203356</td>\n",
       "      <td>-0.157947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.191235</td>\n",
       "      <td>-0.252016</td>\n",
       "      <td>-0.358268</td>\n",
       "      <td>-0.125874</td>\n",
       "      <td>-0.060120</td>\n",
       "      <td>-0.020161</td>\n",
       "      <td>-0.152305</td>\n",
       "      <td>-0.041339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>-0.053785</td>\n",
       "      <td>-0.052899</td>\n",
       "      <td>-0.106212</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.087487</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.020284</td>\n",
       "      <td>0.144724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">60</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.093023</td>\n",
       "      <td>-0.125874</td>\n",
       "      <td>-0.067127</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>0.043564</td>\n",
       "      <td>-0.210526</td>\n",
       "      <td>-0.067127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.132663</td>\n",
       "      <td>-0.112903</td>\n",
       "      <td>-0.275248</td>\n",
       "      <td>-0.112903</td>\n",
       "      <td>-0.093023</td>\n",
       "      <td>-0.119166</td>\n",
       "      <td>-0.339585</td>\n",
       "      <td>-0.125874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.103896</td>\n",
       "      <td>-0.073486</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.086432</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>-0.020161</td>\n",
       "      <td>0.098492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">past</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.079592</td>\n",
       "      <td>-0.171828</td>\n",
       "      <td>-0.119166</td>\n",
       "      <td>-0.053785</td>\n",
       "      <td>-0.086432</td>\n",
       "      <td>0.049213</td>\n",
       "      <td>-0.184158</td>\n",
       "      <td>-0.264224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.112903</td>\n",
       "      <td>-0.106448</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.125874</td>\n",
       "      <td>-0.112903</td>\n",
       "      <td>-0.007968</td>\n",
       "      <td>-0.171828</td>\n",
       "      <td>-0.256207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>-0.007968</td>\n",
       "      <td>0.078156</td>\n",
       "      <td>-0.020161</td>\n",
       "      <td>0.113590</td>\n",
       "      <td>0.109235</td>\n",
       "      <td>0.063555</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>-0.014028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">60</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>0.032064</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>-0.099695</td>\n",
       "      <td>-0.196271</td>\n",
       "      <td>-0.139535</td>\n",
       "      <td>-0.073486</td>\n",
       "      <td>0.037849</td>\n",
       "      <td>-0.131890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.119166</td>\n",
       "      <td>-0.099695</td>\n",
       "      <td>-0.171828</td>\n",
       "      <td>-0.282869</td>\n",
       "      <td>-0.106212</td>\n",
       "      <td>-0.171828</td>\n",
       "      <td>-0.198397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.017875</td>\n",
       "      <td>-0.021718</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.086432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052261</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>-0.060120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">none</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">future</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.003880</td>\n",
       "      <td>-0.009756</td>\n",
       "      <td>-0.021718</td>\n",
       "      <td>-0.086614</td>\n",
       "      <td>-0.264224</td>\n",
       "      <td>-0.203356</td>\n",
       "      <td>-0.139535</td>\n",
       "      <td>-0.125245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.053785</td>\n",
       "      <td>-0.244489</td>\n",
       "      <td>-0.139535</td>\n",
       "      <td>-0.159274</td>\n",
       "      <td>-0.164846</td>\n",
       "      <td>-0.151129</td>\n",
       "      <td>-0.153531</td>\n",
       "      <td>-0.125874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.057942</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>-0.046512</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.032064</td>\n",
       "      <td>-0.126531</td>\n",
       "      <td>0.072581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">60</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.060120</td>\n",
       "      <td>-0.053785</td>\n",
       "      <td>-0.170254</td>\n",
       "      <td>-0.112537</td>\n",
       "      <td>-0.099512</td>\n",
       "      <td>-0.099602</td>\n",
       "      <td>-0.029183</td>\n",
       "      <td>-0.152305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.125874</td>\n",
       "      <td>-0.145418</td>\n",
       "      <td>-0.106212</td>\n",
       "      <td>-0.164846</td>\n",
       "      <td>-0.060120</td>\n",
       "      <td>-0.198397</td>\n",
       "      <td>-0.229703</td>\n",
       "      <td>-0.112903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>-0.033966</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>-0.067127</td>\n",
       "      <td>-0.073486</td>\n",
       "      <td>0.032064</td>\n",
       "      <td>-0.006104</td>\n",
       "      <td>-0.132663</td>\n",
       "      <td>-0.053785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">past</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>0.017875</td>\n",
       "      <td>-0.157947</td>\n",
       "      <td>-0.086614</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>0.029441</td>\n",
       "      <td>0.017875</td>\n",
       "      <td>-0.086614</td>\n",
       "      <td>-0.067698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.125874</td>\n",
       "      <td>-0.186047</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.060120</td>\n",
       "      <td>-0.106212</td>\n",
       "      <td>-0.132663</td>\n",
       "      <td>-0.138614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.129482</td>\n",
       "      <td>0.032064</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>0.037849</td>\n",
       "      <td>0.098492</td>\n",
       "      <td>0.066937</td>\n",
       "      <td>0.134653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">60</th>\n",
       "      <th>L1_logit</th>\n",
       "      <td>-0.020161</td>\n",
       "      <td>-0.119166</td>\n",
       "      <td>-0.041339</td>\n",
       "      <td>-0.119675</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>-0.060120</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>-0.079920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_logit</th>\n",
       "      <td>-0.099602</td>\n",
       "      <td>-0.119166</td>\n",
       "      <td>-0.237052</td>\n",
       "      <td>-0.232558</td>\n",
       "      <td>-0.033966</td>\n",
       "      <td>-0.014028</td>\n",
       "      <td>-0.079920</td>\n",
       "      <td>-0.282869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>0.057942</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>-0.021718</td>\n",
       "      <td>-0.106212</td>\n",
       "      <td>0.057942</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.017875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 results                                \\\n",
       "                                  binary     count  count_sw frequency   \n",
       "lemmas 5min future 1  L1_logit -0.041339 -0.036433 -0.217782 -0.189268   \n",
       "                      L2_logit -0.079920 -0.170254 -0.198397 -0.186047   \n",
       "                      nb        0.144724 -0.105986  0.026210  0.011988   \n",
       "                   60 L1_logit  0.003937 -0.033966 -0.151129 -0.290581   \n",
       "                      L2_logit -0.079920 -0.086432 -0.099602 -0.320792   \n",
       "                      nb       -0.040201  0.057942  0.052261 -0.014028   \n",
       "            past   1  L1_logit -0.229703 -0.329858 -0.215264  0.074583   \n",
       "                      L2_logit -0.267717 -0.271357 -0.279024 -0.237052   \n",
       "                      nb        0.046512 -0.001980 -0.027805 -0.014028   \n",
       "                   60 L1_logit -0.053785 -0.007968  0.029441  0.099804   \n",
       "                      L2_logit -0.099602 -0.186047 -0.093023 -0.067127   \n",
       "                      nb       -0.014028 -0.053785  0.011988  0.011988   \n",
       "       hour future 1  L1_logit -0.067127 -0.079920  0.000000 -0.138614   \n",
       "                      L2_logit -0.125874 -0.125874 -0.125874 -0.132663   \n",
       "                      nb        0.109235 -0.001980  0.032064 -0.132663   \n",
       "                   60 L1_logit -0.125245 -0.145418 -0.067127 -0.033966   \n",
       "                      L2_logit -0.177165 -0.066532 -0.157947 -0.033966   \n",
       "                      nb        0.046512  0.006030  0.052261  0.052261   \n",
       "            past   1  L1_logit -0.015702 -0.041339 -0.222441 -0.119675   \n",
       "                      L2_logit -0.164846 -0.171828 -0.145418 -0.248766   \n",
       "                      nb        0.063555  0.006030  0.063555  0.006030   \n",
       "                   60 L1_logit -0.099602 -0.099602  0.094488 -0.163424   \n",
       "                      L2_logit -0.106212 -0.244489 -0.099602 -0.067127   \n",
       "                      nb        0.052261 -0.014028  0.118952  0.046512   \n",
       "       min  future 1  L1_logit -0.112537  0.007670 -0.164846 -0.093069   \n",
       "                      L2_logit -0.244489 -0.125245 -0.079920 -0.093069   \n",
       "                      nb        0.032064  0.054795  0.011988 -0.047525   \n",
       "                   60 L1_logit -0.040201 -0.203356 -0.020161 -0.226964   \n",
       "                      L2_logit  0.011988 -0.026369 -0.033966 -0.263736   \n",
       "                      nb       -0.027805  0.134653  0.046512 -0.086614   \n",
       "...                                  ...       ...       ...       ...   \n",
       "tokens hour past   1  L1_logit -0.217782  0.083665 -0.194231  0.011988   \n",
       "                      L2_logit -0.066532 -0.139535 -0.041339 -0.033966   \n",
       "                      nb       -0.112903  0.037849  0.063555  0.129482   \n",
       "                   60 L1_logit -0.157947 -0.079920 -0.301887 -0.132663   \n",
       "                      L2_logit -0.152305 -0.060120 -0.093023 -0.171828   \n",
       "                      nb       -0.007968 -0.020161 -0.007968 -0.001980   \n",
       "       min  future 1  L1_logit -0.001980 -0.125874 -0.177165 -0.131890   \n",
       "                      L2_logit -0.191235 -0.252016 -0.358268 -0.125874   \n",
       "                      nb       -0.053785 -0.052899 -0.106212  0.093023   \n",
       "                   60 L1_logit -0.033966 -0.093023 -0.125874 -0.067127   \n",
       "                      L2_logit -0.132663 -0.112903 -0.275248 -0.112903   \n",
       "                      nb        0.103896 -0.073486  0.011988 -0.086432   \n",
       "            past   1  L1_logit -0.079592 -0.171828 -0.119166 -0.053785   \n",
       "                      L2_logit -0.112903 -0.106448 -0.033966 -0.125874   \n",
       "                      nb       -0.007968  0.078156 -0.020161  0.113590   \n",
       "                   60 L1_logit  0.032064 -0.001980 -0.099695 -0.196271   \n",
       "                      L2_logit -0.027805 -0.119166 -0.099695 -0.171828   \n",
       "                      nb        0.017875 -0.021718  0.011988 -0.086432   \n",
       "       none future 1  L1_logit -0.003880 -0.009756 -0.021718 -0.086614   \n",
       "                      L2_logit -0.053785 -0.244489 -0.139535 -0.159274   \n",
       "                      nb        0.057942  0.011988  0.026210 -0.046512   \n",
       "                   60 L1_logit -0.060120 -0.053785 -0.170254 -0.112537   \n",
       "                      L2_logit -0.125874 -0.145418 -0.106212 -0.164846   \n",
       "                      nb       -0.033966  0.006030 -0.067127 -0.073486   \n",
       "            past   1  L1_logit  0.017875 -0.157947 -0.086614  0.011988   \n",
       "                      L2_logit -0.033966 -0.125874 -0.186047 -0.027805   \n",
       "                      nb        0.129482  0.032064 -0.033966 -0.033966   \n",
       "                   60 L1_logit -0.020161 -0.119166 -0.041339 -0.119675   \n",
       "                      L2_logit -0.099602 -0.119166 -0.237052 -0.232558   \n",
       "                      nb        0.057942  0.046512 -0.021718 -0.106212   \n",
       "\n",
       "                                                                           \n",
       "                               log_tfidf log_tfidf_sw     tfidf  tfidf_sw  \n",
       "lemmas 5min future 1  L1_logit -0.132663    -0.035225 -0.086614 -0.093069  \n",
       "                      L2_logit -0.178894    -0.191235 -0.073022 -0.203356  \n",
       "                      nb       -0.014028     0.160243  0.063555 -0.040201  \n",
       "                   60 L1_logit -0.232558    -0.073930 -0.029183 -0.119166  \n",
       "                      L2_logit -0.171828    -0.237052 -0.093069 -0.225126  \n",
       "                      nb       -0.053785     0.000000  0.043564 -0.014028  \n",
       "            past   1  L1_logit -0.198397    -0.196271  0.023692 -0.157947  \n",
       "                      L2_logit -0.159274    -0.145418 -0.066532 -0.191235  \n",
       "                      nb        0.006030    -0.021718  0.032064  0.043564  \n",
       "                   60 L1_logit -0.033966     0.000000 -0.137730 -0.007968  \n",
       "                      L2_logit -0.152305    -0.086432 -0.164846 -0.020161  \n",
       "                      nb        0.072581     0.108163 -0.093069  0.072581  \n",
       "       hour future 1  L1_logit -0.001980    -0.151129 -0.177165 -0.112537  \n",
       "                      L2_logit -0.225126    -0.164846 -0.237052 -0.106212  \n",
       "                      nb       -0.027805    -0.086614  0.011988  0.083665  \n",
       "                   60 L1_logit -0.138614    -0.137730 -0.001980 -0.099602  \n",
       "                      L2_logit -0.232558    -0.171828 -0.112537 -0.177165  \n",
       "                      nb        0.026210     0.049213  0.078156  0.026210  \n",
       "            past   1  L1_logit  0.017875    -0.047525 -0.067127  0.165323  \n",
       "                      L2_logit -0.159274    -0.256207 -0.138614 -0.171828  \n",
       "                      nb        0.149850    -0.026369 -0.073486 -0.014028  \n",
       "                   60 L1_logit -0.086432    -0.105986 -0.093023  0.017875  \n",
       "                      L2_logit -0.099695    -0.093023 -0.014028 -0.132663  \n",
       "                      nb        0.081883     0.026210  0.093023 -0.053785  \n",
       "       min  future 1  L1_logit -0.093069    -0.106212 -0.177165  0.017875  \n",
       "                      L2_logit -0.191235    -0.106212 -0.196271 -0.159274  \n",
       "                      nb       -0.026369    -0.007968 -0.027805 -0.027805  \n",
       "                   60 L1_logit  0.023692    -0.099602 -0.131890 -0.073930  \n",
       "                      L2_logit -0.186047    -0.079920 -0.119166 -0.053785  \n",
       "                      nb        0.057942    -0.027805  0.026210  0.072581  \n",
       "...                                  ...          ...       ...       ...  \n",
       "tokens hour past   1  L1_logit -0.151129    -0.014028 -0.119166 -0.125874  \n",
       "                      L2_logit -0.191235    -0.212982 -0.212982 -0.145418  \n",
       "                      nb       -0.027805    -0.020161  0.011988 -0.053785  \n",
       "                   60 L1_logit  0.046512    -0.137730 -0.132663  0.023692  \n",
       "                      L2_logit -0.093023    -0.106212 -0.145418 -0.119166  \n",
       "                      nb        0.052261     0.118952 -0.079920  0.083665  \n",
       "       min  future 1  L1_logit -0.093069    -0.093069 -0.203356 -0.157947  \n",
       "                      L2_logit -0.060120    -0.020161 -0.152305 -0.041339  \n",
       "                      nb        0.087487     0.046512  0.020284  0.144724  \n",
       "                   60 L1_logit  0.011988     0.043564 -0.210526 -0.067127  \n",
       "                      L2_logit -0.093023    -0.119166 -0.339585 -0.125874  \n",
       "                      nb        0.011988    -0.001980 -0.020161  0.098492  \n",
       "            past   1  L1_logit -0.086432     0.049213 -0.184158 -0.264224  \n",
       "                      L2_logit -0.112903    -0.007968 -0.171828 -0.256207  \n",
       "                      nb        0.109235     0.063555  0.006030 -0.014028  \n",
       "                   60 L1_logit -0.139535    -0.073486  0.037849 -0.131890  \n",
       "                      L2_logit -0.282869    -0.106212 -0.171828 -0.198397  \n",
       "                      nb        0.000000     0.052261  0.003937 -0.060120  \n",
       "       none future 1  L1_logit -0.264224    -0.203356 -0.139535 -0.125245  \n",
       "                      L2_logit -0.164846    -0.151129 -0.153531 -0.125874  \n",
       "                      nb        0.089109     0.032064 -0.126531  0.072581  \n",
       "                   60 L1_logit -0.099512    -0.099602 -0.029183 -0.152305  \n",
       "                      L2_logit -0.060120    -0.198397 -0.229703 -0.112903  \n",
       "                      nb        0.032064    -0.006104 -0.132663 -0.053785  \n",
       "            past   1  L1_logit  0.029441     0.017875 -0.086614 -0.067698  \n",
       "                      L2_logit -0.060120    -0.106212 -0.132663 -0.138614  \n",
       "                      nb        0.037849     0.098492  0.066937  0.134653  \n",
       "                   60 L1_logit -0.001980    -0.060120 -0.001980 -0.079920  \n",
       "                      L2_logit -0.033966    -0.014028 -0.079920 -0.282869  \n",
       "                      nb        0.057942     0.139535  0.103896  0.017875  \n",
       "\n",
       "[96 rows x 8 columns]"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unstack(level=2).xs('kappa', level=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [ 15, 16, 17]])\n",
    "b = np.array(['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "indexes = np.array([0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup \n",
    "class Features(object):\n",
    "\n",
    "    def __init__(self, inputDict):\n",
    "        self.inputs = inputDict\n",
    "        self.price_path = 'AAPL_1min.csv'\n",
    "        self.tweets_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataProcessed\\\\tweetsAAPL.csv'\n",
    "\n",
    "    def load_data(self):\n",
    "        self.tweets = Features.load_tweets(self.tweets_path)\n",
    "        self.prices = Features.load_prices(self.price_path, add_grid = True)\n",
    "        \n",
    "    def load_embeddings(self):\n",
    "        Twitter_200D_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\glove.twitter.27B.200d_edited.txt'\n",
    "        Twitter_200D = gensim.models.KeyedVectors.load_word2vec_format(Twitter_200D_path)\n",
    "        \n",
    "        GoogleNews_300D_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\GoogleNews-vectors-negative300.bin'\n",
    "        GoogleNews_300D = gensim.models.KeyedVectors.load_word2vec_format(GoogleNews_300D_path, binary=True)\n",
    "        \n",
    "        Wikipedia_300D_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\glove.840B.300d.txt'\n",
    "        Wikipedia_300D = gensim.models.KeyedVectors.load_word2vec_format(Wikipedia_300D_path)\n",
    "        \n",
    "        self.embeddings = {'Twitter_200D':Twitter_200D, 'GoogleNews_300D': GoogleNews_300D, 'Wikipedia_300D':Wikipedia_300D}\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_prices(path, add_grid = True):\n",
    "        '''\n",
    "        Loads prices from csv file.\n",
    "        \n",
    "        Returns dataframe with datetime index. Original prices from csv are placed on datetime grid\n",
    "        with one minute frequency over oldest and newest price observations. This is done include After-Hours\n",
    "        price changes - missing prices created by the grid are frontfilled by last valid observations.\n",
    "        \n",
    "        '''\n",
    "        prices = pd.read_csv(path)\n",
    "        prices['DateTime'] = prices['Date'] + ' ' + prices['Time']\n",
    "        prices['DateTime'] = pd.to_datetime(prices['DateTime'])\n",
    "        prices = prices.drop(['Date', 'Time', 'Volume'], axis=1)\n",
    "        prices = prices.set_index('DateTime')\n",
    "                     \n",
    "        if add_grid:\n",
    "            # Create grid\n",
    "            grid_start = min(prices.index) - pd.DateOffset(days=5)\n",
    "            grid_end = max(prices.index) + pd.DateOffset(days=5)\n",
    "            grid = pd.date_range(start=grid_start, end=grid_end, freq='min')\n",
    "            grid = pd.Series(grid).rename('DateTime')\n",
    "            grid = pd.DataFrame(grid).set_index('DateTime')\n",
    "\n",
    "            # Join grid with data\n",
    "            prices = grid.join(prices)\n",
    "            was_NaN = prices['Close'].isnull()\n",
    "            prices = prices.fillna(method = 'ffill')\n",
    "            prices['was_NaN'] = was_NaN\n",
    "        return prices\n",
    "    \n",
    "    @staticmethod    \n",
    "    def load_tweets(path):\n",
    "        '''\n",
    "        Loads preprocessed tweets from csv file.\n",
    "        \n",
    "        Returns multiindexed data frame with 'date', 'hour', '5min' ,'minute', 'id' index levels.\n",
    "        Tweets with identical text occuring more than once per day are assumed to be spamm and are filtered.\n",
    "        \n",
    "        '''\n",
    "        # Load data from csv and convert column lists of words\n",
    "        tweets = pd.read_csv(path)\n",
    "        tweets['lemmas'] = tweets['lemmas'].apply(literal_eval)\n",
    "        tweets['tokens'] = tweets['tokens'].apply(literal_eval)\n",
    "\n",
    "        # Create time variables\n",
    "        tweets['date'] = tweets['created_at'].str[:10]\n",
    "        tweets['hour'] = tweets['created_at'].str[11:13]\n",
    "        tweets['minute'] = tweets['created_at'].str[14:16]\n",
    "        tweets['5min'] = (tweets['minute'].astype(int)//5)*5\n",
    "        \n",
    "        # Spam filtering - Remove duplicate tweets in date\n",
    "        tweets = tweets.drop_duplicates(['date', 'text'])\n",
    "       \n",
    "        # Drop redundant columns and index\n",
    "        tweets = tweets.drop(['Unnamed: 0', 'created_at', 'text'], axis=1)\n",
    "        tweets.set_index(['date', 'hour', '5min' ,'minute', 'id'], inplace = True)\n",
    "        return tweets\n",
    "\n",
    "    \n",
    "    def create_corpuses(self):\n",
    "        self.corpus = {}\n",
    "        self.corpus_list = []\n",
    "        \n",
    "        for form in self.inputs['forms']:\n",
    "            for agg in self.inputs['aggregates']:\n",
    "                corpus_id = (form, agg)\n",
    "                self.corpus_list.append(corpus_id)\n",
    "                \n",
    "                print ('Aggregating: '+ str(corpus_id))\n",
    "                self.corpus[corpus_id] = aggregate_tweets(self.tweets, agg, form)\n",
    "                \n",
    "                \n",
    "    def create_labels(self):\n",
    "        self.label = {}\n",
    "        self.label_list = []\n",
    "        \n",
    "        # Create list of label types\n",
    "        self.label_type_list = []\n",
    "        for direction in self.inputs['directions']:\n",
    "            for window in self.inputs['windows']:\n",
    "                label_type = (direction, window)\n",
    "                self.label_type_list.append(label_type)        \n",
    "        \n",
    "        # Iterate over corpuses and label types\n",
    "        for item in self.corpus_list:\n",
    "            for label_type in self.label_type_list:\n",
    "                label_id = item + label_type\n",
    "                self.label_list.append(label_id)\n",
    "\n",
    "                # Get direction of shift\n",
    "                direction = label_type[0]\n",
    "                window = label_type[1]                \n",
    "                if direction == 'past':\n",
    "                    window_dir = window\n",
    "                elif direction == 'future':\n",
    "                    window_dir = -1*window\n",
    "\n",
    "                # Add label based on window to dataset\n",
    "                self.label[label_id] = get_label(self.corpus[item], self.prices,  window_dir)\n",
    "                    \n",
    "    def create_BOW_dataset(self):\n",
    "        self.BOW_dataset = {}\n",
    "        self.BOW_dataset_list = []\n",
    "        \n",
    "        # Iterate over corpuses\n",
    "        for item in self.corpus_list:\n",
    "            for vec in inputDict['BOW_vectorizers']:\n",
    "                BOW_dataset_id = item + (vec,)\n",
    "                self.BOW_dataset_list.append(BOW_dataset_id)\n",
    "                \n",
    "                # Vectorize text corpus\n",
    "                text = self.corpus[item]['text']\n",
    "                self.BOW_dataset[BOW_dataset_id] = BOW_vectorize(text, vec)\n",
    "\n",
    "    def create_VW_dataset(self):\n",
    "        e.VW_dataset = {}\n",
    "        e.VW_dataset_list = []\n",
    "\n",
    "        # Iterate over corpuses\n",
    "        for item in e.corpus_list:\n",
    "            for emb in inputDict['embeddings']:\n",
    "                for vec in inputDict['WV_vectorizers']:\n",
    "                    dataset_id = item + (emb, vec)\n",
    "                    e.VW_dataset_list.append(dataset_id)\n",
    "\n",
    "                    # Vectorize text corpus\n",
    "                    text = e.corpus[item]['text']\n",
    "                    embedding = e.embeddings[emb]\n",
    "                    e.VW_dataset[dataset_id] = VW_vectorize(text, embedding, vec)                \n",
    "                \n",
    "            \n",
    "    def create_links(self):\n",
    "        self.link = {}\n",
    "        self.link_list = []\n",
    "\n",
    "        # Iterate over corpuses and label types\n",
    "        for item in self.dataset_list:\n",
    "            for label_type in self.label_type_list:\n",
    "                link_id = item + label_type\n",
    "                self.link_list.append(link_id)\n",
    "\n",
    "                # Search for suitable label in self.label\n",
    "                current_label_id = (item[0], item[1]) + label_type\n",
    "                current_label = self.label[current_label_id]\n",
    "\n",
    "                # Get array of indexes without NaN values\n",
    "                index = current_label[current_label['Label'].notnull()].index\n",
    "                self.link[link_id] = {'index': index, 'dataset_id': item, 'label_id': current_label_id}           \n",
    "                \n",
    "                \n",
    "    def evaluate(self):\n",
    "        self.predictions = {}\n",
    "        self.results = {}\n",
    "        \n",
    "        # Iterate over dataset - label pairs\n",
    "        for item in self.link_list:\n",
    "            link = self.link[('lemmas', '5min', 'binary', 'future', 1)]\n",
    "            \n",
    "            # Extract dataset - label pair using links and shuffle \n",
    "            index = link['index']\n",
    "            index = np.random.permutation(index)\n",
    "            dataset = self.dataset[link['dataset_id']][index]\n",
    "            label = self.label[link['label_id']].reindex(index)['Label']\n",
    "            \n",
    "            # Iterate over models\n",
    "            for model in  inputDict['models']:\n",
    "                \n",
    "                # Calculate model predicitons\n",
    "                prediction = get_model_prediction(dataset, label, model)\n",
    "                prediction_id = item + (model,)\n",
    "                self.predictions[prediction_id] = prediction\n",
    "                \n",
    "                # Calculate accuracy and kappa metrics\n",
    "                kappa = cohen_kappa_score(label, prediction)\n",
    "                accuracy = accuracy_score(label, prediction)\n",
    "                \n",
    "                result_id_kappa = item + (model, 'kappa')\n",
    "                result_id_accuracy = item + (model, 'accuracy')\n",
    "                \n",
    "                self.results[result_id_kappa] = kappa\n",
    "                self.results[result_id_accuracy] = accuracy\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
