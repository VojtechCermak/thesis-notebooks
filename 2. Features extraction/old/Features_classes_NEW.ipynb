{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Standard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# word embedings\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Vectorization and evaluation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text as txt\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedings\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Vectorization and evaluation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text as txt\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "def aggregate_tweets(inputDF, freq, forms):\n",
    "    '''\n",
    "    Agregates text over selected frequency.\n",
    "\n",
    "    Selectable frequencies are 'hour', '5min' ,'minute' and 'none' for no aggragating (whole tweets are returned)\n",
    "    Tweets with identical text occuring more than once per day are assumed to be spamm and are filtered.\n",
    "\n",
    "    '''\n",
    "    tweets = inputDF.copy()\n",
    "    special = ['F_exclamation', 'F_question', 'F_ellipsis', 'F_hashtags', 'F_cashtags', 'F_usermention', 'F_urls']\n",
    "\n",
    "    if freq == 'none':\n",
    "        level = ['date', 'hour', '5min', 'minute', 'id']\n",
    "    elif freq == 'min':\n",
    "        level = ['date', 'hour', '5min', 'minute']\n",
    "    elif freq == '5min':\n",
    "        level = ['date', 'hour', '5min']\n",
    "    elif freq == 'hour':\n",
    "        level = ['date', 'hour']\n",
    "    else:\n",
    "        raise ValueError('Frequency is not supported')\n",
    "\n",
    "    # Aggregate tweets and special features\n",
    "    sum_text = tweets[forms].groupby(level=level).apply(sum).rename(\"text\")\n",
    "    sum_special = tweets[special].groupby(level=level).sum().add_prefix('sum')\n",
    "    avg_special = tweets[special].groupby(level=level).mean().add_prefix('avg')\n",
    "    count_tweets = tweets.groupby(level=level).size().rename('tweet_count')\n",
    "    df = pd.concat([sum_special, avg_special, count_tweets, sum_text], axis = 1)\n",
    "\n",
    "    # Reconstruct index to single lablel\n",
    "    df = df.reset_index()\n",
    "    if freq == 'none':\n",
    "        df['DateTime'] = df['date'] + ' ' + df['hour'].astype(str) + ':' + df['minute'].astype(str)\n",
    "        df = df.drop(['date', 'hour', '5min', 'minute', 'id'], axis=1)\n",
    "    elif freq == 'min':\n",
    "        df['DateTime'] = df['date'] + ' ' + df['hour'].astype(str) + ':' + df['minute'].astype(str)\n",
    "        df = df.drop(['date', 'hour', '5min', 'minute'], axis=1)\n",
    "    elif freq == '5min':\n",
    "        df['DateTime'] = df['date'] + ' ' + df['hour'].astype(str) + ':' + df['5min'].astype(str)\n",
    "        df = df.drop(['date', 'hour', '5min'], axis=1)\n",
    "    elif freq == 'hour':\n",
    "        df['DateTime'] = df['date'] + ' ' + df['hour'].astype(str)\n",
    "        df = df.drop(['date', 'hour'], axis=1)\n",
    "    else: \n",
    "        raise ValueError('Frequency is not supported')\n",
    "        \n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])    \n",
    "    df = df.set_index('DateTime')\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_label(textDF, pricesDF, shift):\n",
    "    \"\"\"\n",
    "    shift = n  - label is n minutes lagged\n",
    "    shift = -n  - label is n minute in future\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(pricesDF['Close'])\n",
    "    \n",
    "    if shift > 0 :\n",
    "        df['minLag'] = df['Close'].shift(shift)\n",
    "        conditions = [df['minLag'] == df['Close'], df['minLag'] < df['Close'], df['minLag'] > df['Close']]\n",
    "        df['Label'] = np.select(conditions, ['NoChange', 'Growth', 'Decline'], default='Missing')\n",
    "    else:\n",
    "        df['minShift'] = df['Close'].shift(shift)\n",
    "        conditions = [df['minShift'] == df['Close'], df['minShift'] > df['Close'], df['minShift'] < df['Close']]\n",
    "        df['Label'] = np.select(conditions, ['NoChange', 'Growth', 'Decline'], default='Missing')\n",
    "    \n",
    "    # delete missing label, and also nochange labels if biclass TRUE\n",
    "    df.loc[df['Label'] == 'Missing', 'Label'] = np.nan\n",
    "    df.loc[df['Label'] == 'NoChange', 'Label'] = np.nan\n",
    "        \n",
    "    text_index = pd.DataFrame(index = textDF.index)\n",
    "    labelDF = text_index.join(df)\n",
    "    labelDF = labelDF.reset_index()\n",
    "    \n",
    "    return labelDF\n",
    "\n",
    "def get_model_prediction(inputDF, labeling,  method, validations=5):\n",
    "    if method == 'logit':\n",
    "        model = LogisticRegression(C=1e30,penalty='l2')\n",
    "    elif method == 'L2_logit':\n",
    "        model = LogisticRegression(C=1, penalty='l2')\n",
    "    elif method == 'L1_logit':\n",
    "        model = LogisticRegression(C=1, penalty='l1')\n",
    "    elif method == 'nb':\n",
    "        model = MultinomialNB()\n",
    "    else:\n",
    "        raise ValueError('Method is not supported')\n",
    "    pred = cross_val_predict(model, inputDF, labeling, cv=validations, n_jobs=1, verbose=0)    \n",
    "    return pred     \n",
    "\n",
    "# Vectorization methods\n",
    "def tweet2vec_mean(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            wordVec = embedding[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue   \n",
    "            \n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec = np.zeros(embedding.vector_size)\n",
    "        return tweetVec\n",
    "    \n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "def tweet2vec_minmax(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = embedding[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "            \n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros((embedding.vector_size)*2)\n",
    "        return tweetVec\n",
    "        \n",
    "    minVec = np.min(tweetVec, axis=0)\n",
    "    maxVec = np.max(tweetVec, axis=0)\n",
    "    return np.append(maxVec, minVec)\n",
    "\n",
    "\n",
    "\n",
    "def tweet2vec_mean_sw(tokens, embedding):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            if word not in txt.ENGLISH_STOP_WORDS:\n",
    "                wordVec = embedding[word]\n",
    "                tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "            \n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec = np.zeros(embedding.vector_size)\n",
    "        return tweetVec\n",
    "    \n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "def tweet2vec_tfidf(tokens, embedding, tfidf):\n",
    "    tweetVec = []\n",
    "    weights = []\n",
    "    \n",
    "    vocabulary = tfidf.vocabulary_\n",
    "    idf = tfidf.idf_\n",
    "    \n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = np.array(embedding[word])\n",
    "            weight = idf[vocabulary[word]]\n",
    "            \n",
    "            tweetVec.append(wordVec)\n",
    "            weights.append(weight)\n",
    "        except: continue\n",
    "            \n",
    "    if len(tweetVec) < 1:\n",
    "        tweetVec= np.zeros(embedding.vector_size)\n",
    "        return tweetVec\n",
    "        \n",
    "    weights = weights / np.sum(weights)\n",
    "    tweetVec = np.array(tweetVec)\n",
    "    weighted_vec = tweetVec * weights[:,None]\n",
    "    return weighted_vec.sum(axis = 0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def BOW_vectorize(inputText, method):\n",
    "    '''\n",
    "    Calls scikit text vectorizers based on parameters. Returns sparse matrix. \n",
    "\n",
    "    '''\n",
    "    \n",
    "    if method == 'binary':          # binary terms vectorizer\n",
    "        vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, binary=True)\n",
    "    elif method == 'count':         # Simple count vectorizer\n",
    "        vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, binary=False)\n",
    "    elif method == 'count_sw':      # Simple count vectorizer with stopwords filter\n",
    "        vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, stop_words='english', binary=False)\n",
    "    elif method =='frequency':      # Term frequencies vectorizer\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, sublinear_tf = False, use_idf=False)\n",
    "    elif method =='tfidf':          #simple TFIDF vectorizer\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, sublinear_tf = False, use_idf=True)\n",
    "    elif method =='tfidf_sw':       #simple TFIDF vectorizer with english stop words\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, stop_words='english',sublinear_tf = False, use_idf=True)\n",
    "    elif method =='log_tfidf':      #LOG tf TFIDF vectorizer\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, sublinear_tf = True, use_idf=True)\n",
    "    elif method =='log_tfidf_sw':   #LOG tf TFIDF vectorizer with english stop words\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, stop_words='english', sublinear_tf = True, use_idf=True)\n",
    "    else:\n",
    "        raise ValueError('Method is not supported')\n",
    "    train = vec.fit_transform(inputText)\n",
    "    return train\n",
    "\n",
    "\n",
    "def VW_vectorize(inputText, embedding, method):\n",
    "    # Embedding simple average\n",
    "    if method == 'mean':\n",
    "        df = inputText.apply(tweet2vec_mean, args=[embedding])\n",
    "\n",
    "    # Embedding simple average without stopwords\n",
    "    elif method == 'mean_sw':\n",
    "        df = inputText.apply(tweet2vec_mean_sw, args=[embedding])\n",
    "\n",
    "    # Embedding minimum + maxiumum values concacenated\n",
    "    elif method == 'minmax':\n",
    "        df = inputText.apply(tweet2vec_minmax, args=[embedding])\n",
    "    \n",
    "    # Embedding IDF weighted average\n",
    "    elif method == 'idf':\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "        train = vec.fit_transform(inputText)\n",
    "        df = inputText.apply(tweet2vec_tfidf, args=[embedding, vec])\n",
    "    \n",
    "    # Embedding IDF weighted average without stopwords\n",
    "    elif method == 'idf_sw':\n",
    "        vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, stop_words='english')\n",
    "        train = vec.fit_transform(inputText)\n",
    "        df = inputText.apply(tweet2vec_tfidf, args=[embedding, vec])\n",
    "    else:\n",
    "        raise ValueError('Method is not supported')\n",
    "        \n",
    "    train = df.apply(pd.Series).fillna(0)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features(object):\n",
    "\n",
    "    def __init__(self, inputDict):\n",
    "        self.inputs = inputDict\n",
    "        \n",
    "    def load_data(self):\n",
    "        self.tweets = Features.load_tweets(self.tweets_path)\n",
    "        self.prices = Features.load_prices(self.price_path, add_grid = True)\n",
    "        \n",
    "    def load_embeddings(self):\n",
    "        # loads embeddings to dictionary\n",
    "        self.embeddings = {}\n",
    "        for item in self.embedding_path:\n",
    "            path = self.embedding_path[item]\n",
    "            if path[-4:] == '.bin':\n",
    "                self.embeddings[item] = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "            else:\n",
    "                self.embeddings[item] = gensim.models.KeyedVectors.load_word2vec_format(path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_prices(path, add_grid = True):\n",
    "        '''\n",
    "        Loads prices from csv file.\n",
    "        \n",
    "        Returns dataframe with datetime index. Original prices from csv are placed on datetime grid\n",
    "        with one minute frequency over oldest and newest price observations. This is done include After-Hours\n",
    "        price changes - missing prices created by the grid are frontfilled by last valid observations.\n",
    "        \n",
    "        '''\n",
    "        prices = pd.read_csv(path)\n",
    "        prices['DateTime'] = prices['Date'] + ' ' + prices['Time']\n",
    "        prices['DateTime'] = pd.to_datetime(prices['DateTime'])\n",
    "        prices = prices.drop(['Date', 'Time', 'Volume'], axis=1)\n",
    "        prices = prices.set_index('DateTime')\n",
    "                     \n",
    "        if add_grid:\n",
    "            # Create grid\n",
    "            grid_start = min(prices.index) - pd.DateOffset(days=5)\n",
    "            grid_end = max(prices.index) + pd.DateOffset(days=5)\n",
    "            grid = pd.date_range(start=grid_start, end=grid_end, freq='min')\n",
    "            grid = pd.Series(grid).rename('DateTime')\n",
    "            grid = pd.DataFrame(grid).set_index('DateTime')\n",
    "\n",
    "            # Join grid with data\n",
    "            prices = grid.join(prices)\n",
    "            was_NaN = prices['Close'].isnull()\n",
    "            prices = prices.fillna(method = 'ffill')\n",
    "            prices['was_NaN'] = was_NaN\n",
    "        return prices\n",
    "    \n",
    "    @staticmethod    \n",
    "    def load_tweets(path):\n",
    "        '''\n",
    "        Loads preprocessed tweets from csv file.\n",
    "        \n",
    "        Returns multiindexed data frame with 'date', 'hour', '5min' ,'minute', 'id' index levels.\n",
    "        Tweets with identical text occuring more than once per day are assumed to be spamm and are filtered.\n",
    "        \n",
    "        '''\n",
    "        # Load data from csv and convert column lists of words\n",
    "        tweets = pd.read_csv(path)\n",
    "        tweets['lemmas'] = tweets['lemmas'].apply(literal_eval)\n",
    "        tweets['tokens'] = tweets['tokens'].apply(literal_eval)\n",
    "\n",
    "        # Create time variables\n",
    "        tweets['date'] = tweets['created_at'].str[:10]\n",
    "        tweets['hour'] = tweets['created_at'].str[11:13]\n",
    "        tweets['minute'] = tweets['created_at'].str[14:16]\n",
    "        tweets['5min'] = (tweets['minute'].astype(int)//5)*5\n",
    "        \n",
    "        # Spam filtering - Remove duplicate tweets in date\n",
    "        tweets = tweets.drop_duplicates(['date', 'text'])\n",
    "       \n",
    "        # Drop redundant columns and index\n",
    "        tweets = tweets.drop(['Unnamed: 0', 'created_at', 'text'], axis=1)\n",
    "        tweets.set_index(['date', 'hour', '5min' ,'minute', 'id'], inplace = True)\n",
    "        return tweets\n",
    "\n",
    "    \n",
    "    def create_corpuses(self):\n",
    "        self.corpus = {}\n",
    "        self.corpus_list = []\n",
    "        \n",
    "        for form in self.inputs['forms']:\n",
    "            for agg in self.inputs['aggregates']:\n",
    "                corpus_id = (form, agg)\n",
    "                self.corpus_list.append(corpus_id)\n",
    "                \n",
    "                print ('Aggregating: '+ str(corpus_id))\n",
    "                self.corpus[corpus_id] = aggregate_tweets(self.tweets, agg, form)\n",
    "                \n",
    "                \n",
    "    def create_labels(self):\n",
    "        self.label = {}\n",
    "        self.label_list = []\n",
    "        \n",
    "        # Create list of label types\n",
    "        self.label_type_list = []\n",
    "        for direction in self.inputs['directions']:\n",
    "            for window in self.inputs['windows']:\n",
    "                label_type = (direction, window)\n",
    "                self.label_type_list.append(label_type)        \n",
    "        \n",
    "        # Iterate over corpuses and label types\n",
    "        for item in self.corpus_list:\n",
    "            for label_type in self.label_type_list:\n",
    "                label_id = item + label_type\n",
    "                self.label_list.append(label_id)\n",
    "\n",
    "                # Get direction of shift\n",
    "                direction = label_type[0]\n",
    "                window = label_type[1]                \n",
    "                if direction == 'past':\n",
    "                    window_dir = window\n",
    "                elif direction == 'future':\n",
    "                    window_dir = -1*window\n",
    "\n",
    "                # Add label based on window to dataset\n",
    "                self.label[label_id] = get_label(self.corpus[item], self.prices,  window_dir)\n",
    "                    \n",
    "    def create_BOW_datasets(self):\n",
    "        self.BOW_dataset = {}\n",
    "        self.BOW_dataset_list = []\n",
    "        \n",
    "        # Iterate over corpuses\n",
    "        for item in self.corpus_list:\n",
    "            for vec in inputDict['BOW_vectorizers']:\n",
    "                dataset_id = item + (vec,)\n",
    "                self.BOW_dataset_list.append(dataset_id)\n",
    "                \n",
    "                # Vectorize text corpus\n",
    "                text = self.corpus[item]['text']\n",
    "                self.BOW_dataset[dataset_id] = BOW_vectorize(text, vec)\n",
    "\n",
    "    def create_VW_datasets(self):\n",
    "        self.VW_dataset = {}\n",
    "        self.VW_dataset_list = []\n",
    "\n",
    "        # Iterate over corpuses\n",
    "        for item in self.corpus_list:\n",
    "            for emb in inputDict['embeddings']:\n",
    "                for vec in inputDict['WV_vectorizers']:\n",
    "                    dataset_id = item + (emb, vec)\n",
    "                    self.VW_dataset_list.append(dataset_id)\n",
    "\n",
    "                    # Vectorize text corpus\n",
    "                    text = self.corpus[item]['text']\n",
    "                    embedding = self.embeddings[emb]\n",
    "                    self.VW_dataset[dataset_id] = VW_vectorize(text, embedding, vec)                \n",
    "                \n",
    "            \n",
    "    def create_BOW_links(self):\n",
    "        self.BOW_link = {}\n",
    "        self.BOW_link_list = []\n",
    "\n",
    "        # Iterate over corpuses and label types\n",
    "        for item in self.BOW_dataset_list:\n",
    "            for label_type in self.label_type_list:\n",
    "                link_id = item + label_type\n",
    "                self.BOW_link_list.append(link_id)\n",
    "\n",
    "                # Search for suitable label in self.label\n",
    "                current_label_id = (item[0], item[1]) + label_type\n",
    "                current_label = self.label[current_label_id]\n",
    "\n",
    "                # Get array of indexes without NaN values\n",
    "                index = current_label[current_label['Label'].notnull()].index\n",
    "                self.BOW_link[link_id] = {'index': index, 'dataset_id': item, 'label_id': current_label_id}           \n",
    "\n",
    "    def create_VW_links(self):\n",
    "        self.VW_link = {}\n",
    "        self.VW_link_list = []\n",
    "\n",
    "        # Iterate over corpuses and label types\n",
    "        for item in self.VW_dataset_list:\n",
    "            for label_type in self.label_type_list:\n",
    "                link_id = item + label_type\n",
    "                self.VW_link_list.append(link_id)\n",
    "\n",
    "                # Search for suitable label in self.label\n",
    "                current_label_id = (item[0], item[1]) + label_type\n",
    "                current_label = self.label[current_label_id]\n",
    "\n",
    "                # Get array of indexes without NaN values\n",
    "                index = current_label[current_label['Label'].notnull()].index\n",
    "                self.VW_link[link_id] = {'index': index, 'dataset_id': item, 'label_id': current_label_id}                   \n",
    "                \n",
    "                \n",
    "    def evaluate_BOW(self):\n",
    "        self.BOW_predictions = {}\n",
    "        self.BOW_results = {}\n",
    "        \n",
    "        # Iterate over dataset - label pairs\n",
    "        for item in self.BOW_link_list:\n",
    "            link = self.BOW_link[('lemmas', '5min', 'binary', 'future', 1)]\n",
    "            \n",
    "            # Extract dataset - label pair using links and shuffle \n",
    "            index = link['index']\n",
    "            index = np.random.permutation(index)\n",
    "            dataset = self.BOW_dataset[link['dataset_id']][index]\n",
    "            label = self.label[link['label_id']].reindex(index)['Label']\n",
    "            \n",
    "            # Iterate over models\n",
    "            for model in inputDict['models']:\n",
    "                \n",
    "                # Calculate model predicitons\n",
    "                prediction = get_model_prediction(dataset, label, model)\n",
    "                prediction_id = item + (model,)\n",
    "                self.BOW_predictions[prediction_id] = prediction\n",
    "                \n",
    "                # Calculate accuracy and kappa metrics\n",
    "                kappa = cohen_kappa_score(label, prediction)\n",
    "                accuracy = accuracy_score(label, prediction)\n",
    "                \n",
    "                result_id_kappa = item + (model, 'kappa')\n",
    "                result_id_accuracy = item + (model, 'accuracy')\n",
    "                \n",
    "                self.BOW_results[result_id_kappa] = kappa\n",
    "                self.BOW_results[result_id_accuracy] = accuracy\n",
    "                \n",
    "    def evaluate_VW(self):\n",
    "        self.VW_predictions = {}\n",
    "        self.VW_results = {}\n",
    "        \n",
    "        # Iterate over dataset - label pairs\n",
    "        for item in self.VW_link_list:\n",
    "            link = self.VW_link[('lemmas', '5min', 'binary', 'future', 1)]\n",
    "            \n",
    "            # Extract dataset - label pair using links and shuffle \n",
    "            index = link['index']\n",
    "            index = np.random.permutation(index)\n",
    "            dataset = self.VW_dataset[link['dataset_id']][index]\n",
    "            label = self.label[link['label_id']].reindex(index)['Label']\n",
    "            \n",
    "            # Iterate over models\n",
    "            for model in inputDict['models']:\n",
    "                \n",
    "                # Calculate model predicitons\n",
    "                prediction = get_model_prediction(dataset, label, model)\n",
    "                prediction_id = item + (model,)\n",
    "                self.VW_predictions[prediction_id] = prediction\n",
    "                \n",
    "                # Calculate accuracy and kappa metrics\n",
    "                kappa = cohen_kappa_score(label, prediction)\n",
    "                accuracy = accuracy_score(label, prediction)\n",
    "                \n",
    "                result_id_kappa = item + (model, 'kappa')\n",
    "                result_id_accuracy = item + (model, 'accuracy')\n",
    "                \n",
    "                self.VW_results[result_id_kappa] = kappa\n",
    "                self.VW_results[result_id_accuracy] = accuracy\n",
    "                \n",
    "                print('prediction_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = ['lemmas', 'tokens']\n",
    "aggregates = ['hour', '5min', 'min', 'none']\n",
    "\n",
    "#labels\n",
    "directions = ['past', 'future']\n",
    "windows = [60, 1]\n",
    "\n",
    "embeddings = ['Twitter_200D', 'GoogleNews_300D', 'Wikipedia_300D']\n",
    "WV_vectorizers = ['mean', 'mean_sw', 'minmax', 'idf', 'idf_sw']\n",
    "BOW_vectorizers = ['binary', 'count', 'count_sw', 'frequency', 'tfidf', 'tfidf_sw', 'log_tfidf', 'log_tfidf_sw']\n",
    "\n",
    "\n",
    "# validation\n",
    "models = ['L2_logit', 'L1_logit', 'nb']\n",
    "metrics = ['kappa', 'acc']\n",
    "\n",
    "inputDict = {'forms':forms, 'aggregates':aggregates, 'directions':directions, 'windows':windows, \n",
    "             'BOW_vectorizers':BOW_vectorizers, 'WV_vectorizers':WV_vectorizers, 'embeddings':embeddings,\n",
    "             'models':models, 'metrics':metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced\n",
    "forms = ['tokens']\n",
    "aggregates = ['hour', 'none']\n",
    "\n",
    "#labels\n",
    "directions = ['future']\n",
    "windows = [1]\n",
    "\n",
    "embeddings = ['Wikipedia_300D']\n",
    "WV_vectorizers = ['idf']\n",
    "BOW_vectorizers = ['binary', 'count', 'count_sw', 'frequency', 'tfidf', 'tfidf_sw', 'log_tfidf', 'log_tfidf_sw']\n",
    "\n",
    "\n",
    "# validation\n",
    "models = ['L2_logit', 'L1_logit', 'nb']\n",
    "metrics = ['kappa', 'acc']\n",
    "\n",
    "inputDict = {'forms':forms, 'aggregates':aggregates, 'directions':directions, 'windows':windows, \n",
    "             'BOW_vectorizers':BOW_vectorizers, 'WV_vectorizers':WV_vectorizers, 'embeddings':embeddings,\n",
    "             'models':models, 'metrics':metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " f.embedding_path = {'Twitter_200D':'N:\\\\diplomka temp\\\\word2vec\\\\glove.twitter.27B.200d.txt',\n",
    "                    'GoogleNews_300D': 'N:\\\\diplomka temp\\\\word2vec\\\\GoogleNews-vectors-negative300.bin',\n",
    "                    'Wikipedia_300D':'N:\\\\diplomka temp\\\\word2vec\\\\glove.840B.300d.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = Features(inputDict)\n",
    "f.price_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataMarket\\\\AAPL1min.csv'\n",
    "f.tweets_path = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataProcessed\\\\tweetsAAPL.csv'\n",
    "\n",
    "\n",
    "f.price_path = 'N:\\\\diplomka temp\\\\dataMarket\\\\AAPL1min.csv'\n",
    "f.tweets_path = 'N:\\\\diplomka temp\\\\dataProcessed\\\\tweetsAAPL.csv'\n",
    "f.embedding_path = {'Wikipedia_300D':'N:\\\\diplomka temp\\\\word2vec\\\\glove.840B.300d.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating: ('lemmas', 'hour')\n",
      "Aggregating: ('lemmas', '5min')\n",
      "Aggregating: ('lemmas', 'min')\n",
      "Aggregating: ('lemmas', 'none')\n",
      "Aggregating: ('tokens', 'hour')\n",
      "Aggregating: ('tokens', '5min')\n",
      "Aggregating: ('tokens', 'min')\n",
      "Aggregating: ('tokens', 'none')\n",
      "Wall time: 53min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f.load_data()\n",
    "f.create_corpuses()\n",
    "f.create_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f.create_BOW_datasets()\n",
    "f.create_BOW_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstraping epoch:   0\n",
      "Bootstraping epoch:   1\n",
      "Bootstraping epoch:   2\n",
      "Bootstraping epoch:   3\n",
      "Bootstraping epoch:   4\n",
      "Bootstraping epoch:   5\n",
      "Bootstraping epoch:   6\n",
      "Bootstraping epoch:   7\n",
      "Wall time: 19h 9min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(8):\n",
    "    f.evaluate_BOW()\n",
    "    results[i] = f.BOW_results\n",
    "    print('Bootstraping epoch:   ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lemmas', 'hour'),\n",
       " ('lemmas', '5min'),\n",
       " ('lemmas', 'min'),\n",
       " ('lemmas', 'none'),\n",
       " ('tokens', 'hour'),\n",
       " ('tokens', '5min'),\n",
       " ('tokens', 'min'),\n",
       " ('tokens', 'none')]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f.info = 'AAPL data set with loaded BOW type datasets. Includes loaded corpuses, loaded BOW datasets, BOW links and labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filehandler = open(\"dataset_AAPL_BOW\",\"wb\")\n",
    "pickle.dump(f,filehandler)\n",
    "filehandler.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.load_pickle()\n",
    "        self.create_dataframe()\n",
    "        \n",
    "    def load_pickle(self):\n",
    "        file = open(self.path,'rb')\n",
    "        self.dict_results = pickle.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    def create_dataframe(self):\n",
    "        self.dataframes = {}\n",
    "        for i in self.dict_results:\n",
    "            dataframe = Results.dict_to_dataframe(results[i])\n",
    "            dataframe = dataframe.rename(columns = {'results':'run-' + str(i)})\n",
    "            self.dataframes[i] = dataframe\n",
    "            \n",
    "        self.df = pd.concat([self.dataframes[i] for i in self.dataframes], axis = 1)    \n",
    "            \n",
    "    @staticmethod\n",
    "    def dict_to_dataframe(input_dict):\n",
    "        # Convert dictionary to dataframe\n",
    "        dict_items = input_dict.items()\n",
    "        df = pd.DataFrame(list(dict_items))\n",
    "\n",
    "        # Add index\n",
    "        index = pd.MultiIndex.from_tuples(df[0])\n",
    "        df = df.drop(0, axis = 1)\n",
    "        df = df.rename(columns = {1:'results'})\n",
    "        df = df.set_index(index)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = Results('results_BOW_APPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<393759x73341 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5176978 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.BOW_dataset['tokens', 'none', 'tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones((300, 393759), dtype = np.float32)\n",
    "y = np.random.rand(300,393759)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0].itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float16"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(['nb', 'L1_logit'], level = -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = pd.DataFrame(df2.mean(axis = 1))\n",
    "std = pd.DataFrame(df2.std(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>results</th>\n",
       "      <th>results</th>\n",
       "      <th>results</th>\n",
       "      <th>results</th>\n",
       "      <th>results</th>\n",
       "      <th>results</th>\n",
       "      <th>results</th>\n",
       "      <th>results</th>\n",
       "      <th>results</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>hour</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.510282</td>\n",
       "      <td>0.500162</td>\n",
       "      <td>0.506765</td>\n",
       "      <td>0.502544</td>\n",
       "      <td>0.506440</td>\n",
       "      <td>0.503626</td>\n",
       "      <td>0.501515</td>\n",
       "      <td>0.505682</td>\n",
       "      <td>0.506765</td>\n",
       "      <td>0.502489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>min</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.503518</td>\n",
       "      <td>0.511581</td>\n",
       "      <td>0.501786</td>\n",
       "      <td>0.501245</td>\n",
       "      <td>0.504113</td>\n",
       "      <td>0.505953</td>\n",
       "      <td>0.505899</td>\n",
       "      <td>0.501786</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>0.507685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>5min</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.500649</td>\n",
       "      <td>0.502381</td>\n",
       "      <td>0.511311</td>\n",
       "      <td>0.503193</td>\n",
       "      <td>0.502489</td>\n",
       "      <td>0.502489</td>\n",
       "      <td>0.500325</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.499729</td>\n",
       "      <td>0.499242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>hour</th>\n",
       "      <th>frequency</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.503139</td>\n",
       "      <td>0.503464</td>\n",
       "      <td>0.503626</td>\n",
       "      <td>0.511473</td>\n",
       "      <td>0.505791</td>\n",
       "      <td>0.505087</td>\n",
       "      <td>0.502273</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.501028</td>\n",
       "      <td>0.502977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>none</th>\n",
       "      <th>count</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.501515</td>\n",
       "      <td>0.504979</td>\n",
       "      <td>0.505412</td>\n",
       "      <td>0.505141</td>\n",
       "      <td>0.509092</td>\n",
       "      <td>0.503301</td>\n",
       "      <td>0.505845</td>\n",
       "      <td>0.504005</td>\n",
       "      <td>0.506115</td>\n",
       "      <td>0.499784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">tokens</th>\n",
       "      <th>min</th>\n",
       "      <th>binary</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.505195</td>\n",
       "      <td>0.501461</td>\n",
       "      <td>0.501461</td>\n",
       "      <td>0.507685</td>\n",
       "      <td>0.504925</td>\n",
       "      <td>0.511203</td>\n",
       "      <td>0.499351</td>\n",
       "      <td>0.501786</td>\n",
       "      <td>0.503409</td>\n",
       "      <td>0.500758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.503139</td>\n",
       "      <td>0.502435</td>\n",
       "      <td>0.505358</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.502868</td>\n",
       "      <td>0.503572</td>\n",
       "      <td>0.512014</td>\n",
       "      <td>0.504113</td>\n",
       "      <td>0.503409</td>\n",
       "      <td>0.503680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5min</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.507901</td>\n",
       "      <td>0.498431</td>\n",
       "      <td>0.504438</td>\n",
       "      <td>0.502544</td>\n",
       "      <td>0.501191</td>\n",
       "      <td>0.503031</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.511473</td>\n",
       "      <td>0.503626</td>\n",
       "      <td>0.506386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemmas</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">5min</th>\n",
       "      <th>count</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.499675</td>\n",
       "      <td>0.506332</td>\n",
       "      <td>0.503734</td>\n",
       "      <td>0.505899</td>\n",
       "      <td>0.501028</td>\n",
       "      <td>0.508605</td>\n",
       "      <td>0.507306</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>0.511094</td>\n",
       "      <td>0.499296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.503031</td>\n",
       "      <td>0.506981</td>\n",
       "      <td>0.502868</td>\n",
       "      <td>0.502977</td>\n",
       "      <td>0.502381</td>\n",
       "      <td>0.504384</td>\n",
       "      <td>0.503842</td>\n",
       "      <td>0.500379</td>\n",
       "      <td>0.503193</td>\n",
       "      <td>0.510228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    results   results  \\\n",
       "lemmas hour count_sw  future 60 L2_logit accuracy  0.510282  0.500162   \n",
       "tokens min  count_sw  past   1  L2_logit accuracy  0.503518  0.511581   \n",
       "lemmas 5min tfidf     future 60 L2_logit accuracy  0.500649  0.502381   \n",
       "tokens hour frequency past   60 L2_logit accuracy  0.503139  0.503464   \n",
       "lemmas none count     past   60 L2_logit accuracy  0.501515  0.504979   \n",
       "tokens min  binary    past   60 L2_logit accuracy  0.505195  0.501461   \n",
       "       none tfidf     future 1  L2_logit accuracy  0.503139  0.502435   \n",
       "       5min tfidf     past   1  L2_logit accuracy  0.507901  0.498431   \n",
       "lemmas 5min count     past   60 L2_logit accuracy  0.499675  0.506332   \n",
       "            log_tfidf future 60 L2_logit accuracy  0.503031  0.506981   \n",
       "\n",
       "                                                    results   results  \\\n",
       "lemmas hour count_sw  future 60 L2_logit accuracy  0.506765  0.502544   \n",
       "tokens min  count_sw  past   1  L2_logit accuracy  0.501786  0.501245   \n",
       "lemmas 5min tfidf     future 60 L2_logit accuracy  0.511311  0.503193   \n",
       "tokens hour frequency past   60 L2_logit accuracy  0.503626  0.511473   \n",
       "lemmas none count     past   60 L2_logit accuracy  0.505412  0.505141   \n",
       "tokens min  binary    past   60 L2_logit accuracy  0.501461  0.507685   \n",
       "       none tfidf     future 1  L2_logit accuracy  0.505358  0.500000   \n",
       "       5min tfidf     past   1  L2_logit accuracy  0.504438  0.502544   \n",
       "lemmas 5min count     past   60 L2_logit accuracy  0.503734  0.505899   \n",
       "            log_tfidf future 60 L2_logit accuracy  0.502868  0.502977   \n",
       "\n",
       "                                                    results   results  \\\n",
       "lemmas hour count_sw  future 60 L2_logit accuracy  0.506440  0.503626   \n",
       "tokens min  count_sw  past   1  L2_logit accuracy  0.504113  0.505953   \n",
       "lemmas 5min tfidf     future 60 L2_logit accuracy  0.502489  0.502489   \n",
       "tokens hour frequency past   60 L2_logit accuracy  0.505791  0.505087   \n",
       "lemmas none count     past   60 L2_logit accuracy  0.509092  0.503301   \n",
       "tokens min  binary    past   60 L2_logit accuracy  0.504925  0.511203   \n",
       "       none tfidf     future 1  L2_logit accuracy  0.502868  0.503572   \n",
       "       5min tfidf     past   1  L2_logit accuracy  0.501191  0.503031   \n",
       "lemmas 5min count     past   60 L2_logit accuracy  0.501028  0.508605   \n",
       "            log_tfidf future 60 L2_logit accuracy  0.502381  0.504384   \n",
       "\n",
       "                                                    results   results  \\\n",
       "lemmas hour count_sw  future 60 L2_logit accuracy  0.501515  0.505682   \n",
       "tokens min  count_sw  past   1  L2_logit accuracy  0.505899  0.501786   \n",
       "lemmas 5min tfidf     future 60 L2_logit accuracy  0.500325  0.504762   \n",
       "tokens hour frequency past   60 L2_logit accuracy  0.502273  0.500974   \n",
       "lemmas none count     past   60 L2_logit accuracy  0.505845  0.504005   \n",
       "tokens min  binary    past   60 L2_logit accuracy  0.499351  0.501786   \n",
       "       none tfidf     future 1  L2_logit accuracy  0.512014  0.504113   \n",
       "       5min tfidf     past   1  L2_logit accuracy  0.500974  0.511473   \n",
       "lemmas 5min count     past   60 L2_logit accuracy  0.507306  0.501894   \n",
       "            log_tfidf future 60 L2_logit accuracy  0.503842  0.500379   \n",
       "\n",
       "                                                    results   results  \n",
       "lemmas hour count_sw  future 60 L2_logit accuracy  0.506765  0.502489  \n",
       "tokens min  count_sw  past   1  L2_logit accuracy  0.501894  0.507685  \n",
       "lemmas 5min tfidf     future 60 L2_logit accuracy  0.499729  0.499242  \n",
       "tokens hour frequency past   60 L2_logit accuracy  0.501028  0.502977  \n",
       "lemmas none count     past   60 L2_logit accuracy  0.506115  0.499784  \n",
       "tokens min  binary    past   60 L2_logit accuracy  0.503409  0.500758  \n",
       "       none tfidf     future 1  L2_logit accuracy  0.503409  0.503680  \n",
       "       5min tfidf     past   1  L2_logit accuracy  0.503626  0.506386  \n",
       "lemmas 5min count     past   60 L2_logit accuracy  0.511094  0.499296  \n",
       "            log_tfidf future 60 L2_logit accuracy  0.503193  0.510228  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc[df2.idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = mean.xs('kappa', level=-1) / std.xs('kappa', level=-1) \n",
    "distr = kappa[0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (mean.xs('accuracy', level=-1) - 0.5) / std.xs('accuracy', level=-1)\n",
    "acc[0].sort_values(ascending = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mean.xs('kappa', level=-1)[0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>5min</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.010762</td>\n",
       "      <td>1.706539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>5min</th>\n",
       "      <th>log_tfidf_sw</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.010266</td>\n",
       "      <td>1.589775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>hour</th>\n",
       "      <th>tfidf_sw</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.010184</td>\n",
       "      <td>2.078129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">tokens</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">none</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.010009</td>\n",
       "      <td>1.612114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009885</td>\n",
       "      <td>2.271938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binary</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009884</td>\n",
       "      <td>1.697924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">lemmas</th>\n",
       "      <th>hour</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009843</td>\n",
       "      <td>1.842244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5min</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009817</td>\n",
       "      <td>1.971246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <th>frequency</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009800</td>\n",
       "      <td>1.959825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009761</td>\n",
       "      <td>1.949928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>hour</th>\n",
       "      <th>binary</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009742</td>\n",
       "      <td>1.998879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>none</th>\n",
       "      <th>log_tfidf_sw</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009719</td>\n",
       "      <td>1.425358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">tokens</th>\n",
       "      <th>none</th>\n",
       "      <th>binary</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009634</td>\n",
       "      <td>2.972116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <th>count</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009584</td>\n",
       "      <td>2.715874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <th>count</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009553</td>\n",
       "      <td>2.282693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">lemmas</th>\n",
       "      <th>min</th>\n",
       "      <th>count</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009385</td>\n",
       "      <td>1.855129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <th>tfidf_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009380</td>\n",
       "      <td>1.927020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5min</th>\n",
       "      <th>log_tfidf_sw</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009369</td>\n",
       "      <td>2.290963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>min</th>\n",
       "      <th>frequency</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009356</td>\n",
       "      <td>3.433172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemmas</th>\n",
       "      <th>hour</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009273</td>\n",
       "      <td>2.684096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <th>log_tfidf_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009212</td>\n",
       "      <td>1.132748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>hour</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009189</td>\n",
       "      <td>1.912079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>min</th>\n",
       "      <th>log_tfidf_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009182</td>\n",
       "      <td>1.690179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tokens</th>\n",
       "      <th>hour</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009122</td>\n",
       "      <td>2.442468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <th>binary</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009108</td>\n",
       "      <td>1.973919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>min</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009055</td>\n",
       "      <td>1.639196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>min</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.009013</td>\n",
       "      <td>1.850699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemmas</th>\n",
       "      <th>none</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.008988</td>\n",
       "      <td>1.735857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.008967</td>\n",
       "      <td>1.451354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">tokens</th>\n",
       "      <th>hour</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.008890</td>\n",
       "      <td>1.765054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <th>log_tfidf_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004982</td>\n",
       "      <td>1.087405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <th>frequency</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004859</td>\n",
       "      <td>1.558798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>min</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.969773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tokens</th>\n",
       "      <th>none</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004787</td>\n",
       "      <td>0.847264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.739729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>5min</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004774</td>\n",
       "      <td>0.584453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>hour</th>\n",
       "      <th>binary</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.678456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>hour</th>\n",
       "      <th>log_tfidf_sw</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004670</td>\n",
       "      <td>0.771845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>5min</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004604</td>\n",
       "      <td>0.788231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">lemmas</th>\n",
       "      <th>min</th>\n",
       "      <th>count</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004603</td>\n",
       "      <td>0.810029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5min</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.768022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004561</td>\n",
       "      <td>0.612752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>hour</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004436</td>\n",
       "      <td>1.366519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>none</th>\n",
       "      <th>frequency</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004337</td>\n",
       "      <td>0.800276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tokens</th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004308</td>\n",
       "      <td>1.495450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004271</td>\n",
       "      <td>1.195516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">lemmas</th>\n",
       "      <th>hour</th>\n",
       "      <th>binary</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.584991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <th>tfidf_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004145</td>\n",
       "      <td>1.158162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>future</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.004125</td>\n",
       "      <td>0.582844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5min</th>\n",
       "      <th>count</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.885856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tokens</th>\n",
       "      <th>5min</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.003890</td>\n",
       "      <td>1.049514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.607115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">lemmas</th>\n",
       "      <th>none</th>\n",
       "      <th>tfidf_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.607739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5min</th>\n",
       "      <th>count_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.003363</td>\n",
       "      <td>0.627933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_tfidf_sw</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.863810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>none</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.431793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <th>5min</th>\n",
       "      <th>count</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.629376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th>min</th>\n",
       "      <th>log_tfidf</th>\n",
       "      <th>past</th>\n",
       "      <th>60</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.389415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">lemmas</th>\n",
       "      <th>hour</th>\n",
       "      <th>frequency</th>\n",
       "      <th>past</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.709463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <th>binary</th>\n",
       "      <th>future</th>\n",
       "      <th>1</th>\n",
       "      <th>L2_logit</th>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.405862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0         x\n",
       "lemmas 5min tfidf        future 1  L2_logit  0.010762  1.706539\n",
       "tokens 5min log_tfidf_sw future 60 L2_logit  0.010266  1.589775\n",
       "lemmas hour tfidf_sw     future 1  L2_logit  0.010184  2.078129\n",
       "tokens none count_sw     future 60 L2_logit  0.010009  1.612114\n",
       "            tfidf        future 60 L2_logit  0.009885  2.271938\n",
       "            binary       future 1  L2_logit  0.009884  1.697924\n",
       "lemmas hour tfidf        future 60 L2_logit  0.009843  1.842244\n",
       "       5min tfidf        past   60 L2_logit  0.009817  1.971246\n",
       "       min  frequency    past   1  L2_logit  0.009800  1.959825\n",
       "       hour log_tfidf    future 1  L2_logit  0.009761  1.949928\n",
       "tokens hour binary       past   1  L2_logit  0.009742  1.998879\n",
       "lemmas none log_tfidf_sw future 1  L2_logit  0.009719  1.425358\n",
       "tokens none binary       past   1  L2_logit  0.009634  2.972116\n",
       "       min  count        future 1  L2_logit  0.009584  2.715874\n",
       "       none count        past   1  L2_logit  0.009553  2.282693\n",
       "lemmas min  count        future 60 L2_logit  0.009385  1.855129\n",
       "       none tfidf_sw     past   60 L2_logit  0.009380  1.927020\n",
       "       5min log_tfidf_sw future 1  L2_logit  0.009369  2.290963\n",
       "tokens min  frequency    past   60 L2_logit  0.009356  3.433172\n",
       "lemmas hour log_tfidf    future 60 L2_logit  0.009273  2.684096\n",
       "       min  log_tfidf_sw past   1  L2_logit  0.009212  1.132748\n",
       "tokens hour log_tfidf    past   1  L2_logit  0.009189  1.912079\n",
       "lemmas min  log_tfidf_sw past   60 L2_logit  0.009182  1.690179\n",
       "tokens hour log_tfidf    past   60 L2_logit  0.009122  2.442468\n",
       "       min  binary       future 60 L2_logit  0.009108  1.973919\n",
       "lemmas min  count_sw     past   60 L2_logit  0.009055  1.639196\n",
       "tokens min  tfidf        future 60 L2_logit  0.009013  1.850699\n",
       "lemmas none log_tfidf    past   1  L2_logit  0.008988  1.735857\n",
       "       hour count_sw     future 60 L2_logit  0.008967  1.451354\n",
       "tokens hour log_tfidf    future 1  L2_logit  0.008890  1.765054\n",
       "...                                               ...       ...\n",
       "       min  log_tfidf_sw past   1  L2_logit  0.004982  1.087405\n",
       "       none frequency    future 60 L2_logit  0.004859  1.558798\n",
       "lemmas min  log_tfidf    past   1  L2_logit  0.004817  0.969773\n",
       "tokens none count_sw     past   1  L2_logit  0.004787  0.847264\n",
       "       hour tfidf        past   60 L2_logit  0.004785  0.739729\n",
       "lemmas 5min count_sw     future 60 L2_logit  0.004774  0.584453\n",
       "tokens hour binary       past   60 L2_logit  0.004676  0.678456\n",
       "lemmas hour log_tfidf_sw future 1  L2_logit  0.004670  0.771845\n",
       "tokens 5min log_tfidf    future 1  L2_logit  0.004604  0.788231\n",
       "lemmas min  count        past   1  L2_logit  0.004603  0.810029\n",
       "       5min count_sw     future 1  L2_logit  0.004598  0.768022\n",
       "       none count_sw     past   1  L2_logit  0.004561  0.612752\n",
       "tokens hour tfidf        future 1  L2_logit  0.004436  1.366519\n",
       "lemmas none frequency    future 1  L2_logit  0.004337  0.800276\n",
       "tokens hour count        past   60 L2_logit  0.004308  1.495450\n",
       "       none log_tfidf    past   60 L2_logit  0.004271  1.195516\n",
       "lemmas hour binary       future 60 L2_logit  0.004249  0.584991\n",
       "       min  tfidf_sw     past   60 L2_logit  0.004145  1.158162\n",
       "       none tfidf        future 60 L2_logit  0.004125  0.582844\n",
       "       5min count        future 1  L2_logit  0.003951  0.885856\n",
       "tokens 5min count_sw     past   1  L2_logit  0.003890  1.049514\n",
       "       none log_tfidf    past   1  L2_logit  0.003770  0.607115\n",
       "lemmas none tfidf_sw     past   1  L2_logit  0.003738  0.607739\n",
       "       5min count_sw     past   60 L2_logit  0.003363  0.627933\n",
       "            log_tfidf_sw past   60 L2_logit  0.003338  0.863810\n",
       "tokens none tfidf        past   1  L2_logit  0.003132  0.431793\n",
       "lemmas 5min count        past   1  L2_logit  0.003127  0.629376\n",
       "tokens min  log_tfidf    past   60 L2_logit  0.002659  0.389415\n",
       "lemmas hour frequency    past   1  L2_logit  0.002537  0.709463\n",
       "       min  binary       future 1  L2_logit  0.002253  0.405862\n",
       "\n",
       "[256 rows x 2 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([x, distr], axis = 1).sort_values(ascending = False, by = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = distr.rename('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vec = VW_vectorize(txt, f.embeddings['Wikipedia_300D'], 'idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f.create_VW_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f.create_VW_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f.evaluate_VW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating: ('lemmas', 'hour')\n",
      "Aggregating: ('lemmas', '5min')\n",
      "Aggregating: ('lemmas', 'min')\n",
      "Aggregating: ('lemmas', 'none')\n",
      "Aggregating: ('tokens', 'hour')\n",
      "Aggregating: ('tokens', '5min')\n",
      "Aggregating: ('tokens', 'min')\n",
      "Aggregating: ('tokens', 'none')\n"
     ]
    }
   ],
   "source": [
    "e = Features(inputDict)\n",
    "e.tweets = f.tweets[:1000]\n",
    "e.prices = f.prices\n",
    "e.embeddings = f.embeddings\n",
    "\n",
    "\n",
    "e.create_corpuses()\n",
    "e.create_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.create_BOW_datasets()\n",
    "e.create_BOW_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.create_VW_datasets()\n",
    "e.create_VW_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e.VW_link_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VW_vectorize(text, embedding, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = f.BOW_results\n",
    "y = pd.DataFrame(list(x.items()))\n",
    "index = pd.MultiIndex.from_tuples(y[0])\n",
    "y = y.drop(0, axis = 1)\n",
    "y = y.rename(columns = {1:'results'})\n",
    "y = y.set_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results    (tokens, none, log_tfidf, past, 60, L2_logit)\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.xs('accuracy', level=-1).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "results  binary          0.003938\n",
       "         count           0.004315\n",
       "         count_sw        0.003735\n",
       "         frequency       0.004824\n",
       "         log_tfidf       0.003838\n",
       "         log_tfidf_sw    0.002889\n",
       "         tfidf           0.004427\n",
       "         tfidf_sw        0.002490\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unstack(level=2).xs('kappa', level=-1).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
