{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from utils import *\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "market_GOOG = load_prices(\"C:\\\\Users\\\\Vojta-Acer\\Desktop\\\\Diplomka\\\\dataMarket\\\\GOOG.csv\")\n",
    "tweets_GOOG = load_tweets(\"C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataProcessed\\\\tweetsGOOG.csv\")\n",
    "tweets_GOOG_5min = aggregate_tweets(tweets_GOOG, '5min', 'tokens')\n",
    "tweets_AAPL = load_tweets(\"C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataProcessed\\\\tweetsAAPL.csv\")\n",
    "tweets_AAPL_5min = aggregate_tweets(tweets_AAPL, '5min', 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\diplomka temp\\\\word2vec\\\\glove.twitter.27B.200d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-36384bf18050>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load Embedding data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mW2V\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E:\\diplomka temp\\word2vec\\glove.twitter.27B.200d.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0memb_5min_GOOG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_GOOG_5min\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet2vec_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mW2V\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0memb_5min_GOOG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memb_5min_GOOG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0memb_5min_AAPL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_AAPL_5min\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet2vec_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mW2V\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's3u'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[1;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m     \u001b[0mraw_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\diplomka temp\\\\word2vec\\\\glove.twitter.27B.200d.txt'"
     ]
    }
   ],
   "source": [
    "# Load Embedding data\n",
    "W2V = gensim.models.KeyedVectors.load_word2vec_format(\"E:\\diplomka temp\\word2vec\\glove.twitter.27B.200d.txt\")\n",
    "emb_5min_GOOG = tweets_GOOG_5min['text'].apply(tweet2vec_mean, args=[W2V])\n",
    "emb_5min_GOOG = emb_5min_GOOG.apply(pd.Series)\n",
    "emb_5min_AAPL = tweets_AAPL_5min['text'].apply(tweet2vec_mean, args=[W2V])\n",
    "emb_5min_AAPL = emb_5min_GOOG.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating labels from market data\n",
    "\n",
    "First of all, we restrict market data to trading hours and bussiness days, otherwise we would get bias because both tweets and stocks follow the same weed and day/night cycles. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#market = market_master.copy()\n",
    "#market = market_master[market_master['Tick Count'] > 0]\n",
    "market_GOOG['TC 5min'] = market_GOOG['Tick Count'].rolling(5).mean()\n",
    "market_GOOG['TC 60min'] = market_GOOG['Tick Count'].rolling(60).mean()\n",
    "market = market_GOOG.between_time('9:30', '16:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "market['Tick Count'].plot(use_index=False, title = 'Tick count for each minute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots of tick count shows, that there is still some seasonality present. Significant positive outlyers are last minutes of trading day.\n",
    "\n",
    "\n",
    "## Goals of this section is to create labels from market data\n",
    "ideas for labels, we can model many different things, such as:\n",
    "\n",
    "#### Change in price\n",
    "Try to model change up/down of prices after/before some time\n",
    "- failed, just noise is there\n",
    "\n",
    "#### Up Tick minus Down Tick\n",
    "\n",
    "###### Construction\n",
    "First, we subtrack down tick from uptick on minute frequency. This value is then split to three categories, based on its quantiles: 0 - 0.33 - 0.66 - 1. Upside of this approach to labeling is having three - category label with equal class distribution as result. Then, we **add lag/lead to the label** and we are ready for modeling.\n",
    "\n",
    "###### Modeling\n",
    "In the example, we use 100 minutes of lag to the label. We use twitter minute data as text data. \n",
    "\n",
    "Vectorizer is simple Count vectorizer, which is about twice as better (cca 0.01 Kappa) as log TF-IDF with smoothing, which should be the best vectorizer according to IR theory (it has 0.005 Kappa)\n",
    "Used model is Multinomial NB, as it is fast and reasonably good with high dim data\n",
    "\n",
    "Since we have equally distributed labels to classes, accuracy is good measure as well\n",
    "\n",
    "- Results: only cca 1% kappa, so it can be noise\n",
    "- Results: Results are NOT CHANGING with adding lag or lead to the label (!!!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = (market_GOOG['Up Ticks'] - market_GOOG['Down Ticks']).rename('label')\n",
    "label.index = label.index + pd.DateOffset(minutes = 10)\n",
    "features = tweets_GOOG_5min.join(label, how = 'inner').dropna() # we use Twitter 5 minute data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features['label'] = pd.qcut(features['label'], [0, 0.25, 0.66, 1.])  # three category label\n",
    "features = features.sample(frac=1) # shuffle\n",
    "\n",
    "\n",
    "# Modeling\n",
    "vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, binary=False)\n",
    "label = features['label'].astype(str)\n",
    "train = {}\n",
    "train['Modeling using text'] = vec.fit_transform(features['text'])\n",
    "train['Modeling using twitter count'] = pd.DataFrame(features['tweet_count'])\n",
    "train['Modeling using date/hour/minute'] = pd.DataFrame([features.index.hour.astype(str),\n",
    "                                                   features.index.minute.astype(str), \n",
    "                                                   features.index.dayofweek.astype(str)]).T\n",
    "for key in train:\n",
    "    pred = cross_val_predict(MultinomialNB(), train[key], label, cv=5, n_jobs=1, verbose=0)\n",
    "    print(key)\n",
    "    evaluate(pred, label)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tick Count\n",
    "\n",
    "###### Construction\n",
    "Similar to previous, but we use Total Tick count in given minute to construct the categories. Then similar as previous.\n",
    "\n",
    "Value is split to four categories, based on quantiles: 0 - 0.25 - 0.5 - 0.75 - 1. Upside of this approach to labeling is having four - category label with equal class distribution as result. Then, we **add lag/lead to the label** and we are ready for modeling.\n",
    "\n",
    "\n",
    "###### Modeling\n",
    "Again, we use 100 minutes of lag to the label with twitter minute data as text data. Vectorizer is log TF IDF with smoothing (here is slightly better).\n",
    "\n",
    "Used model is Multinomial NB, as it is fast and reasonably good with high dim data\n",
    "\n",
    "- Results: Reasonable accuracy and Kappa, even in the 4 category setting (3 category is even better)\n",
    "- Experiments: lower twitter frequency means lower kappa/accuracy\n",
    "- Vectorizers are sensitive to terrible settings (for example that filtering 90% observations means bad results)\n",
    "- Again, with adding lag or lead to the label the results are NOT CHANGING as expected: With greater temporal distance from tweet, the label is predicted with better accuracy (!!!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = market['Tick Count'].rename('label')\n",
    "label.index = label.index + pd.DateOffset(minutes = 600)\n",
    "\n",
    "features = tweets_5min.join(label, how = 'left').dropna() # we use Twitter minute data\n",
    "features['label'] = pd.qcut(features['label'], [0, .25, .5, .75, 1.])  # four category label\n",
    "features = features.sample(frac=1) # shuffle\n",
    "#features = features.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Modeling\n",
    "vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, sublinear_tf = True, use_idf=True)\n",
    "label = features['label'].astype(str)\n",
    "train = {}\n",
    "train['Modeling using text'] = vec.fit_transform(features['text'])\n",
    "train['Modeling using twitter count'] = pd.DataFrame(features['tweet_count'])\n",
    "train['Modeling using date/hour/minute'] = pd.DataFrame([features.index.hour.astype(str),\n",
    "                                                   features.index.minute.astype(str), \n",
    "                                                   features.index.dayofweek.astype(str)]).T\n",
    "for key in train:\n",
    "    pred = cross_val_predict(MultinomialNB(), train[key], label, cv=5, n_jobs=1, verbose=0)\n",
    "    print(key)\n",
    "    evaluate(pred, label)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Adjustment\n",
    "df = pd.DataFrame(market['TC 5min'].rename('label'))\n",
    "df['day'] = df.index.dayofweek\n",
    "df['day'] = df.groupby(by = 'day')['label'].transform(np.median)\n",
    "\n",
    "df['hour'] = df.index.hour\n",
    "df['hour'] = df.groupby(by = 'hour')['label'].transform(np.median)\n",
    "\n",
    "df['label_adj'] = df['label'] - df['hour']\n",
    "target = df['label_adj']\n",
    "target.index = target.index + pd.DateOffset(days = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = market['TC 5min'].rename('label')\n",
    "target.index = target.index + pd.DateOffset(days = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, label = tweets_AAPL_5min.align(target, axis=0, join='inner')\n",
    "train, label = shuffle(train, label)\n",
    "#train = shuffle(train)\n",
    "\n",
    "vecorizer = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "train = vecorizer.fit_transform(train['text'].values)\n",
    "model = Ridge(alpha = 2)\n",
    "\n",
    "pred = cross_val_predict(model, train, label, cv=3)\n",
    "mean_squared_error(label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(label)\n",
    "y['pred'] = pred\n",
    "y = y.sort_index()\n",
    "y.index = y.index.astype(str)\n",
    "y.rolling(50).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adjustment\n",
    "df = pd.DataFrame(market['TC 5min'].rename('label'))\n",
    "df['day'] = df.index.dayofweek\n",
    "df['day'] = df.groupby(by = 'day')['label'].transform(np.median)\n",
    "\n",
    "df['hour'] = df.index.hour\n",
    "df['hour'] = df.groupby(by = 'hour')['label'].transform(np.median)\n",
    "\n",
    "df['label_adj'] = df['label'] - df['hour']\n",
    "target = df['label_adj']\n",
    "target.index = target.index + pd.DateOffset(hours = 240)\n",
    "\n",
    "train, label = tweets_5min.align(target, axis=0, join='inner')\n",
    "vecorizer = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "train = vecorizer.fit_transform(train['text'].values)\n",
    "\n",
    "d = np.ones(0)\n",
    "model = Ridge(alpha = 5)\n",
    "window = 10\n",
    "for i in range(1000, 12000, window):\n",
    "    model.fit(train[:i], label[:i])\n",
    "    pred = model.predict(train[i:i+window])\n",
    "    d = np.append(d, pred)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(model.coef_)\n",
    "inverse = dict((v, k) for k, v in vecorizer.vocabulary_.items())\n",
    "\n",
    "x = list(coef.nlargest(80).index)\n",
    "#x = list(coef.nsmallest(20).index)\n",
    "for item in x:\n",
    "    print(inverse[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(label[1000:12000])\n",
    "y['predictions'] = d\n",
    "y.index = y.index.astype(str)\n",
    "y.rolling(10).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(label[1000:12010])\n",
    "y['predictions'] = d\n",
    "y.index = y.index.astype(str)\n",
    "y.rolling(500).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(label[1000:12000])\n",
    "y['predictions'] = d\n",
    "y.index = y.index.astype(str)\n",
    "y.rolling(200).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train[:1000], label[:1000])\n",
    "pred = model.predict(train[1000:1000+i])\n",
    "df = pd.DataFrame(label[1000:1000+i])\n",
    "df['prediction'] = pred\n",
    "\n",
    "df.sort_index().rolling(100).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embbedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = market['TC 5min'].rename('label')\n",
    "target.index = target.index + pd.DateOffset(hours = 5)\n",
    "\n",
    "train, label = emb_5min_A.align(target, axis=0, join='inner')\n",
    "#train = shuffle(train)\n",
    "train, label = shuffle(train, label)\n",
    "\n",
    "model = Ridge(alpha = 0.1)\n",
    "pred = cross_val_predict(model, train, label, cv=5)\n",
    "mean_squared_error(label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(label)\n",
    "y['pred'] = pred\n",
    "y = y.sort_index()\n",
    "y.index = y.index.astype(str)\n",
    "y.rolling(100).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(label, x['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(label.index)\n",
    "x['mean'] = label.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = {}\n",
    "for i in list(range(-300, 300, 1)):\n",
    "\n",
    "    target = market['TC 5min'].rename('label')\n",
    "    target.index = target.index + pd.DateOffset(days = i)\n",
    "\n",
    "    train, label = emb_5min_GOOG.align(target, axis=0, join='inner')\n",
    "    train, label = shuffle(train, label)\n",
    "\n",
    "    model = Ridge(alpha = 5)\n",
    "    pred = cross_val_predict(model, train, label, cv=5)\n",
    "    g[i] = mean_squared_error(label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(g, index = [0]).T\n",
    "y = y.reindex(y.index[::-1])\n",
    "y.index = -1*(y.index)\n",
    "\n",
    "plt.plot(y[0], color = 'black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(label)\n",
    "y['pred'] = pred\n",
    "y = y.sort_index()\n",
    "y.index = y.index.astype(str)\n",
    "y.rolling(300).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = market['Close'].rolling(5).std().rename('label')\n",
    "#label = market['Close'].rename('label')\n",
    "\n",
    "label.index = label.index + pd.DateOffset(minutes = 10)\n",
    "label = label.replace([np.inf, -np.inf], np.nan)\n",
    "features = tweets_5min.join(label, how = 'left').dropna()\n",
    "#features = features.sample(frac=1, replace = False) # shuffle\n",
    "\n",
    "# Modeling\n",
    "vec = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, sublinear_tf = True, use_idf=True)\n",
    "#vec = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, binary=False)\n",
    "\n",
    "\n",
    "label = features['label']\n",
    "train = vec.fit_transform(features['text'])\n",
    "\n",
    "#model = LinearRegression()\n",
    "model = Ridge(alpha = 1)\n",
    "\n",
    "pred = cross_val_predict(model, train, label, cv=5, n_jobs=1, verbose=0)\n",
    "mean_squared_error(label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(label)\n",
    "\n",
    "y['pred'] = pred\n",
    "y = y.sort_index()\n",
    "y.index = y.index.astype(str)\n",
    "y.rolling(100).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.rolling(1).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tweet_count'] = np.log(dataset['tweet_count'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
