{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# file manipulation\n",
    "import os\n",
    "import json\n",
    "\n",
    "#text manipulation\n",
    "import nltk\n",
    "import re as regex\n",
    "\n",
    "# Tokenize and Lemmatize text\n",
    "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "# Load dictionary with contractions\n",
    "with open(\"data/contractions.json\") as f:\n",
    "    dic = json.load(f)\n",
    "    \n",
    "# Replace contractions with full words\n",
    "def replace_contraction(text, dic = dic):\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def remove_urls(tweets):\n",
    "    regexp = regex.compile(r\"http.?://[^\\s]+[\\s]?\")\n",
    "    replace_by = ' '\n",
    "    tweets.loc[:, \"text\"].replace(regexp, replace_by, inplace=True)\n",
    "    return tweets\n",
    "\n",
    "def remove_user(tweets):\n",
    "    regexp = regex.compile(r'@[^\\s]+[\\s]?')\n",
    "    replace_by = ' '\n",
    "    tweets.loc[:, \"text\"].replace(regexp, replace_by, inplace=True)\n",
    "    return tweets\n",
    "\n",
    "def remove_special(tweets):\n",
    "    regexp = regex.compile(u'[^A-Za-z0-9]+')\n",
    "    replace_by = ' '\n",
    "    tweets.loc[:, \"text\"].replace(regexp, replace_by, inplace=True)\n",
    "    return tweets\n",
    "\n",
    "def remove_numbers(tweets):\n",
    "    regexp = regex.compile(r'\\s?[0-9]+\\.?[0-9]*')\n",
    "    replace_by = ''\n",
    "    tweets.loc[:, \"text\"].replace(regexp, replace_by, inplace=True)\n",
    "    return tweets\n",
    "\n",
    "def remove_spaces(tweets):\n",
    "    regexp = regex.compile(r'\\s\\s+')\n",
    "    replace_by = ' '\n",
    "    tweets.loc[:, \"text\"].replace(regexp, replace_by, inplace=True)\n",
    "    #Remove spaces at the beginning and end\n",
    "    tweets['text'] = tweets['text'].str.strip()\n",
    "    return tweets\n",
    "\n",
    "\n",
    "# helper Functions to Convert Penn PoS tag to WordNet PoS tag standard\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "# Convert Penn to WordNet\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "# Tokenize and Lemmatize text\n",
    "def lemmatize(txt, wnl = WordNetLemmatizer(), tkn = TreebankWordTokenizer):\n",
    "    '''    \n",
    "     1.) We first tokenize and tag part of speech (PoS)\n",
    "     2.) Lemmatize PoS:  adjectives, nouns, verbs, adverbs. \n",
    "     3.) Drop PoS from further analysis: connections, articles, prepositions, etc.\n",
    "    '''\n",
    "    lemmaList = [wnl.lemmatize(i,penn_to_wn(j)) for i,j in pos_tag(tkn().tokenize(txt)) if penn_to_wn(j) in ['a','n','v','r']]\n",
    "    return lemmaList\n",
    "\n",
    "def stemming(txt, ps = PorterStemmer(), tkn = TreebankWordTokenizer):\n",
    "    '''    \n",
    "     1.) We first tokenize sentense\n",
    "     2.) apply porter Stemmer for each word and return list\n",
    "    '''\n",
    "    stemList = [ps.stem(word) for word in tkn().tokenize(txt)]\n",
    "    return stemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Load tweets from JSON\n",
    "# Get path to twitter folder\n",
    "DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataRaw\\\\AAPL'\n",
    "#DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataRaw\\\\MSFT'\n",
    "#DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataRaw\\\\NFLX'\n",
    "#DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataRaw\\\\TSLA'\n",
    "#DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataRaw\\\\GOOG'\n",
    "\n",
    "# Get files inside the folder\n",
    "DataList = os.listdir(DataPath)\n",
    "\n",
    "TweetsData = []\n",
    "# Load Twitter data\n",
    "for item in DataList:\n",
    "    TweetsPath = DataPath + '\\\\' + item\n",
    "\n",
    "    with open(TweetsPath, \"r\", errors='ignore') as TweetsFile:\n",
    "        for line in TweetsFile:\n",
    "            try:\n",
    "                Tweet = json.loads(line)\n",
    "                TweetsData.append(Tweet)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove corrupted observation in case of GOOGLE\n",
    "TweetsData[219426] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 27min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Load Data frame of tweets\n",
    "# Convert loaded tweets to dataframe with selected columns\n",
    "tweets = pd.DataFrame(TweetsData)\n",
    "tweets = tweets[['id','created_at', 'text',  'lang', 'retweeted_status', 'quoted_status']]\n",
    "\n",
    "# Convert date to datetime\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'], format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "# drop corupted observations\n",
    "tweets = tweets.dropna(subset=['id', 'text', 'created_at', 'lang'], how='all')\n",
    "\n",
    "# remove non-english tweets, retweets, quotes \n",
    "tweets = tweets.loc[tweets['retweeted_status'].isnull()]\n",
    "tweets = tweets.loc[tweets['quoted_status'].isnull()]\n",
    "tweets = tweets.loc[tweets['lang'] == 'en']\n",
    "\n",
    "# additional features\n",
    "tweets['F_exclamation'] = tweets['text'].str.count(\"!\")\n",
    "tweets['F_question'] = tweets['text'].str.count(\"\\?\")\n",
    "tweets['F_ellipsis'] = tweets['text'].str.count(r\"\\.\\s?\\.\\s?\\.\")\n",
    "tweets['F_hashtags'] = tweets['text'].str.count(\"#\")\n",
    "tweets['F_cashtags'] = tweets['text'].str.count(\"\\$\")\n",
    "tweets['F_usermention'] = tweets['text'].str.count(\"@\")\n",
    "tweets['F_urls'] = tweets['text'].str.count(r\"http.?://[^\\s]+[\\s]?\")\n",
    "\n",
    "# Drop useless columns\n",
    "droplist = [\"lang\",\"retweeted_status\", \"quoted_status\"]\n",
    "tweets = tweets.drop(droplist, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 10min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Preprocess text\n",
    "tweetsClean = tweets.copy()\n",
    "\n",
    "#1.) Lowercase all characters\n",
    "tweetsClean['text'] = tweetsClean['text'].str.lower()\n",
    "\n",
    "#2.) Replace contractions\n",
    "tweetsClean['text'] = tweetsClean['text'].apply(replace_contraction)\n",
    "\n",
    "tweetsClean = remove_urls(tweetsClean) #3.) Remove URL\n",
    "tweetsClean = remove_user(tweetsClean) #4.) Remove @USER\n",
    "tweetsClean = remove_special(tweetsClean) #5.) Remove special characters\n",
    "tweetsClean = remove_numbers(tweetsClean) #6.) Remove numbers\n",
    "tweetsClean = remove_spaces(tweetsClean) #7.) Remove redundant spaces\n",
    "\n",
    "#8.) get lemmas from tweets\n",
    "tweetsClean['lemmas'] = tweetsClean['text'].apply(lemmatize)\n",
    "\n",
    "#9.) get lemmas from tweets\n",
    "tweetsClean['stems'] = tweetsClean['text'].apply(stemming)\n",
    "\n",
    "#10.) get tokens from tweets\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tweetsClean['tokens'] = tweetsClean['text'].apply(tokenizer.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "tweetsClean.to_csv('tweetsAAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
