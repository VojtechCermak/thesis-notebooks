{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Standard\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# file manipulation\n",
    "import os\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "# word embedings\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Word Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import TSNE\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization methods\n",
    "def Tweet2Vec_averaging(tokens):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            wordVec = vectorization.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue            \n",
    "    return np.mean(tweetVec, axis=0)\n",
    "\n",
    "def Tweet2Vec_min(tokens):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = vectorization.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "    return np.min(tweetVec, axis=0)\n",
    "\n",
    "def Tweet2Vec_max(tokens):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            wordVec = vectorization.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "    return np.max(tweetVec, axis=0)\n",
    "\n",
    "def Tweet2Vec_minmax(tokens):\n",
    "    tweetVec = []\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = vectorization.wv[word]\n",
    "            tweetVec.append(wordVec)\n",
    "        except: continue\n",
    "\n",
    "    minVec = np.min(tweetVec, axis=0)\n",
    "    maxVec = np.max(tweetVec, axis=0)\n",
    "    return np.append(maxVec, minVec)\n",
    "\n",
    "def Tweet2Vec_tfidf(tokens):\n",
    "    tweetVec = []\n",
    "    weightSum = 0\n",
    "    for word in tokens:\n",
    "        try:        \n",
    "            wordVec = vectorization.wv[word]\n",
    "            weight = idf[vocabulary[word]]\n",
    "            \n",
    "            weightSum = weightSum + weight\n",
    "            tweetVec.append(wordVec*weight)\n",
    "        except: continue            \n",
    "    return np.mean(tweetVec, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1f40f1cdaba0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Load W2V (Google News trained)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mDataPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mW2Vmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Load word Embedings\n",
    "# Load Glove (twitter trained)\n",
    "DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\glove.twitter.27B.200d.txt'\n",
    "GLOVEmodel = gensim.models.KeyedVectors.load_word2vec_format(DataPath)\n",
    "\n",
    "# Load W2V (Google News trained)\n",
    "DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\word2vec\\\\GoogleNews-vectors-negative300.bin'\n",
    "W2Vmodel = gensim.models.KeyedVectors.load_word2vec_format(DataPath, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load tweets to pandas\n",
    "DataPath = 'C:\\\\Users\\\\Vojta-Acer\\\\Desktop\\\\Diplomka\\\\dataProcessed\\\\tweetsAAPL.csv'\n",
    "tweetsClean = pd.read_csv(DataPath)\n",
    "# convert column values to lists of words\n",
    "tweetsClean['lemmas'] = tweetsClean['lemmas'].apply(literal_eval)\n",
    "tweetsClean['tokens'] = tweetsClean['tokens'].apply(literal_eval)\n",
    "\n",
    "## Index tweets\n",
    "tweets = tweetsClean.copy()\n",
    "\n",
    "# create time variables and filter spam\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'], format='%Y-%m-%d %H:%M:%S')\n",
    "tweets['date'] = tweets['created_at'].astype(str).str[:10]\n",
    "tweets['hour'] = tweets['created_at'].astype(str).str[11:13]\n",
    "tweets['minute'] = tweets['created_at'].astype(str).str[14:16]\n",
    "tweets['5min'] = (tweets['minute'].astype(int)//5)*5\n",
    "tweets = tweets.drop_duplicates(['date', 'text'])\n",
    "\n",
    "#Reindex\n",
    "tweets.set_index(['date', 'hour', '5min' ,'minute', 'id'], inplace = True)\n",
    "\n",
    "# learn TF IDF on tokens\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, stop_words='english', min_df = 5)\n",
    "tfidf_matrix = tfidf.fit_transform(tweets['tokens'])\n",
    "\n",
    "# get  IDF dictionary\n",
    "vocabulary = tfidf.vocabulary_\n",
    "idf = tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example tweets\n",
    "exampleTweets = pd.read_csv('tweetsExamplesSelection.csv')\n",
    "#exampleTweets = pd.read_csv('tweetsExamplesSelection.csv')\n",
    "#exampleTweets = pd.read_csv('tweetsExamplesFull.csv')\n",
    "\n",
    "colorDict = {'A':'red', 'B':'blue', 'C':'green', 'D':'black', 'E':'purple', 'F':'brown', 'G':'yellow', 'H':'orange'}\n",
    "colorSeries = exampleTweets['Color'].apply(lambda x: colorDict[x])\n",
    "classes = exampleTweets['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select method of embedding vectorization\n",
    "\n",
    "vectorization = GLOVEmodel\n",
    "#vectorization = W2Vmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exampleTweets['Message'].apply(Tweet2Vec_averaging)\n",
    "twitterVec = df.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2> TSNE Plot using Twitter trained Word2Vec<h2/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = exampleTweets['Message'].apply(Tweet2Vec_averaging)\n",
    "twitterVec = df.apply(pd.Series)\n",
    "color_to_marker = {'red': u'o','green': u'o','yellow':u'+', 'brown': u'+'}\n",
    "markerSeries = colorSeries.map(pd.Series(color_to_marker))\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=6, n_iter=12000, random_state=7)\n",
    "#tsne = TSNE(n_components=2, verbose=1, perplexity=3, n_iter=12000, random_state=7)\n",
    "\n",
    "tsne_results = tsne.fit_transform(twitterVec.values)\n",
    "\n",
    "x = tsne_results[:,0]\n",
    "y = tsne_results[:,1] \n",
    "\n",
    "class_id_convertor={'Encryption fight':1, \n",
    "                    'iStores issues in India':2,\n",
    "                    'iPhone sales in India  ':3,\n",
    "                    'Buffet buys Apple shares':4}\n",
    "\n",
    "inv_class_id_convertor = {v: k for k, v in class_id_convertor.items()}\n",
    "class_id = classes.map(class_id_convertor)\n",
    "df = pd.DataFrame({'x': x, 'y':y, 'class_name': classes, 'class_id': class_id})\n",
    "groups = df.groupby('class_id')\n",
    "\n",
    "color_to_marker = {1: 'o',2: '^',3:'*', 4: 's'}\n",
    "color_to_color = {1: 'C3',2: 'C2',3:'blue', 4: 'C8'}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 5)\n",
    "ax.margins(0.05)\n",
    "for name, group in groups:\n",
    "    marker = color_to_marker[name]\n",
    "    color = color_to_color[name]\n",
    "    label = inv_class_id_convertor[name]\n",
    "    ax.plot(group.x, group.y, marker=marker, color = color, linestyle='', ms=8, label=label)\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"Tweet vectorization - Mean of word vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exampleTweets['Message'].apply(Tweet2Vec_tfidf)\n",
    "twitterVec = df.apply(pd.Series)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "#tsne = TSNE(n_components=2, verbose=1, perplexity=8, n_iter=12000, random_state=7)\n",
    "#tsne = TSNE(n_components=2, verbose=1, perplexity=4, n_iter=12000, random_state=7)\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=4, n_iter=12000, random_state=7)\n",
    "\n",
    "tsne_results = tsne.fit_transform(twitterVec.values)\n",
    "\n",
    "x = tsne_results[:,0]\n",
    "y = tsne_results[:,1] \n",
    "\n",
    "class_id_convertor={'Encryption fight':1, \n",
    "                    'iStores issues in India':2,\n",
    "                    'iPhone sales in India  ':3,\n",
    "                    'Buffet buys Apple shares':4}\n",
    "\n",
    "inv_class_id_convertor = {v: k for k, v in class_id_convertor.items()}\n",
    "class_id = classes.map(class_id_convertor)\n",
    "df = pd.DataFrame({'x': x, 'y':y, 'class_name': classes, 'class_id': class_id})\n",
    "groups = df.groupby('class_id')\n",
    "\n",
    "color_to_marker = {1: 'o',2: '^',3:'*', 4: 's'}\n",
    "color_to_color = {1: 'C3',2: 'C2',3:'blue', 4: 'C8'}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 5)\n",
    "ax.margins(0.05)\n",
    "for name, group in groups:\n",
    "    marker = color_to_marker[name]\n",
    "    color = color_to_color[name]\n",
    "    label = inv_class_id_convertor[name]\n",
    "    ax.plot(group.x, group.y, marker=marker, color = color, linestyle='', ms=8, label=label, )\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=5)\n",
    "plt.title(\"IDF weighted mean word vectors\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TSNE Plots Combined <h2/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE\n",
    "fig, ax = plt.subplots(1,2, figsize = (16, 6.5))\n",
    "\n",
    "# PLOT 1\n",
    "df = exampleTweets['Message'].apply(Tweet2Vec_averaging)\n",
    "twitterVec = df.apply(pd.Series)\n",
    "color_to_marker = {'red': u'o','green': u'o','yellow':u'+', 'brown': u'+'}\n",
    "markerSeries = colorSeries.map(pd.Series(color_to_marker))\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=6, n_iter=12000, random_state=7)\n",
    "tsne_results = tsne.fit_transform(twitterVec.values)\n",
    "x = tsne_results[:,0]\n",
    "y = tsne_results[:,1] \n",
    "\n",
    "class_id_convertor={'Encryption fight':1, \n",
    "                    'iStores issues in India':2,\n",
    "                    'iPhone sales in India  ':3,\n",
    "                    'Buffet buys Apple shares':4}\n",
    "inv_class_id_convertor = {v: k for k, v in class_id_convertor.items()}\n",
    "class_id = classes.map(class_id_convertor)\n",
    "df = pd.DataFrame({'x': x, 'y':y, 'class_name': classes, 'class_id': class_id})\n",
    "groups = df.groupby('class_id')\n",
    "color_to_marker = {1: 'o',2: '^',3:'*', 4: 's'}\n",
    "color_to_color = {1: 'C3',2: 'C2',3:'blue', 4: 'C8'}\n",
    "ax[0].margins(0.05)\n",
    "for name, group in groups:\n",
    "    marker = color_to_marker[name]\n",
    "    color = color_to_color[name]\n",
    "    label = inv_class_id_convertor[name]\n",
    "    ax[0].plot(group.x, group.y, marker=marker, color = color, linestyle='', ms=8, label=label)\n",
    "ax[0].legend(loc='upper center', bbox_to_anchor=(1.1, -0.1), ncol=4)\n",
    "ax[0].set_title(\"Mean of word vectors\")\n",
    "\n",
    "# PLOT 2\n",
    "df = exampleTweets['Message'].apply(Tweet2Vec_tfidf)\n",
    "twitterVec = df.apply(pd.Series)\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=4, n_iter=12000, random_state=7)\n",
    "tsne_results = tsne.fit_transform(twitterVec.values)\n",
    "x = tsne_results[:,0]\n",
    "y = tsne_results[:,1] \n",
    "class_id_convertor={'Encryption fight':1, \n",
    "                    'iStores issues in India':2,\n",
    "                    'iPhone sales in India  ':3,\n",
    "                    'Buffet buys Apple shares':4}\n",
    "inv_class_id_convertor = {v: k for k, v in class_id_convertor.items()}\n",
    "class_id = classes.map(class_id_convertor)\n",
    "df = pd.DataFrame({'x': x, 'y':y, 'class_name': classes, 'class_id': class_id})\n",
    "groups = df.groupby('class_id')\n",
    "color_to_marker = {1: 'o',2: '^',3:'*', 4: 's'}\n",
    "color_to_color = {1: 'C3',2: 'C2',3:'blue', 4: 'C8'}\n",
    "ax[1].margins(0.05)\n",
    "for name, group in groups:\n",
    "    marker = color_to_marker[name]\n",
    "    color = color_to_color[name]\n",
    "    label = inv_class_id_convertor[name]\n",
    "    ax[1].plot(group.x, group.y, marker=marker, color = color, linestyle='', ms=8, label=label, )\n",
    "ax[1].set_title(\"IDF weighted mean of word vectors\")\n",
    "fig.savefig('visualisation_glove_vectorization.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
